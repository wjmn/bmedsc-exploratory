#+TITLE: Lab Notebook
#+AUTHOR: Jamin Wu

Note: initially, several of the entries in this notebook were kept in
separate files. They have now been merged into this file and as a
result, there may be some slight inconsistencies in formatting or how
the entries are laid out (particularly during March, which are much
less technical than the remaining entries). This serves as a scrapbook
more than anything else. 

I combined my writing on <2019-06-11 Tue> and as a result, entries
before this time may be incomplete. 

* March 2019
** <2019-03-04 Mon>
:LOGBOOK:
CLOCK: [2019-03-05 Tue 10:11]--[2019-03-05 Tue 10:12] =>  0:01
CLOCK: [2019-03-04 Mon 11:34]--[2019-03-04 Mon 19:29] =>  7:55
:END:

Today's the first official day of honours. 

I got a key with a $20 deposit (MAKE SURE TO BRING IT BACK) for the
neurobionics lab.

Potential projects:

1. Psychophysics
2. Image processing
3. Eye movements? - eye tracking for calibrating cortical input on movement
4. Visual cortex organisation - V2 is more abstract than V1 or how else done?
5. Ancillary inputs - auditory/visual perception as adjuncts, so e.g. could "ask
   for a cup" and have the visual system highlight it or the like.
6. Attention - how does attention change what gets perceived for the visual
   prosthesis?
7. Cortical space representation and phosphene accuracy

I've also tried setting up =org-ref= and =interleave= on my laptop so I can
start to make notes for my literature review and the like.

** <2019-03-05 Tue>
:LOGBOOK:
CLOCK: [2019-03-05 Tue 15:00]--[2019-03-05 Tue 16:38] =>  1:38
CLOCK: [2019-03-05 Tue 10:12]--[2019-03-05 Tue 14:27] =>  4:15
:END:

Been going through the Chen 2008 two part papers (/Simulating Prosthetic
Vision/). They're really quite helpful and lay out some good information on the
topic.

Found a paper on transformative reality - seems like a very
interesting angle.

In terms of directions, here are some things I can think of:

- Expanding on transformative reality - using machine learning algorithms to
  highlight objects when requested, possibly providing adjunct devices (e.g.
  microphone input, audio feedback, haptic feedback) to augment the visual
  experience (something similar to an "accio")
- Could be implemented alongisde "mode switching" e.g. as per Lui et al's
  paper - could be implemented as things like structural edge mode, depth floor
  mode, face mode, text mode, etc.

** <2019-03-06 Wed>
:LOGBOOK:
CLOCK: [2019-03-06 Wed 12:05]--[2019-03-06 Wed 14:05] =>  2:00
CLOCK: [2019-03-06 Wed 12:01]--[2019-03-06 Wed 12:05] =>  0:04
:END:

Ideas:

- Edge detection + smart filling of selected objects
- Audio adjunct to the device.

Todos:
- Collect physiology room key and claim a desk
- Figure out a rough timeline and proposal for the project
- Start working on literature review

I think I'd like to read some more papers on the image processing methods used
for the bionic eye.

But I think as along as I can get a timeline for things, I should have a bit
more structure about what to do.

So here's a draft timeline:

*** Goal Timeline
**** March

  - Decide on testable experiment and focus of honours project
  - Begin prototyping processing algorithms
  - Complete half of literature review

**** April


  - Conduct survey(?) to identify areas of most benefit for visually impaired patients
  - Design experiment to test processing algorithms
  - [2019-04-17 Wed] - Progress report due - discussed and signed by supervisor
  - By end April - Complete draft literature review

**** May

  - Recruit participants for experiment
  - [2019-05-01 Wed] - Department Oral No. 1
  - [2019-05-15 Wed] - Literature Review due

**** June

  - Finalise processing algorithms to test

**** July

  - Conduct experiment and reiterate if necessary
  - Preliminary analysis of results

**** August

  - Analyse results
  - Work on thesis

**** September

  - Work on thesis
  - [2019-09-25 Wed] - Department oral No. 2
  - By end of September - draft thesis

**** October

  - [2019-10-29 Tue] - Thesis Due
  - [2019-10-29 Tue] - Faculty Oral and Presentation upload to Moodle Due
  - [2019-10-30 Wed] - Faculty Oral and Presentation Days

** <2019-03-07 Thu>
:LOGBOOK:
CLOCK: [2019-03-07 Thu 22:25]--[2019-03-07 Thu 22:28] =>  0:03
CLOCK: [2019-03-07 Thu 11:48]--[2019-03-07 Thu 12:48] =>  1:00
CLOCK: [2019-03-07 Thu 09:29]--[2019-03-07 Thu 11:48] =>  2:19
:END:

I ran a quick search on Scopus using:

- visual AND prosthe?i?, AND
- intracortical OR cortical, AND
- image AND processing

Found quite a few interesting articles that would be worth reading.

Picked up the key from physiology today. Didn't have to deposit anything, just
wrote down my name on a piece of paper and they gave it to me. I suppose Ill
have to just remember to return it at the end of the year since they have the record.

Going to try doing some quick prototyping in MatLab. Looks like I'll need a few
toolboxes - installed the webcam one, going to install the image processing one
too. 

** <2019-03-08 Fri>
:LOGBOOK:
CLOCK: [2019-03-08 Fri 13:31]--[2019-03-08 Fri 13:38] =>  0:07
CLOCK: [2019-03-08 Fri 09:59]--[2019-03-08 Fri 13:31] =>  3:32
:END:

So I've started looking at Matlab. I downloaded the Computer Vision toolbox and
finally worked out how to get some the input and output for "real-time"
processing using the webcam (had to use =vision.videoPlayer=).

Spent a bit of time getting a basic simulation loop going in Matlab. I
guess Matlab isn't /that/ terrible...it was quite fast and easy to
prototype.  I'm thinking I could implement the foveal/more dense at
the center by using a different kernel based on the position in the
matrix (I guess I'd be making a 2 + 2 = 4 dimensional kernel matrix,
ah, then you could just multiply it out - though I suppose the
dimensions would have to agree) - but in such a case, the kernels at
the edges would have central values that are quite "broad" and hence
give the phosphene a larger "size" e.g.

Near the center:

| 0 |  1 | 0 |
| 1 | 10 | 1 |
| 0 |  1 | 0 |


And near the peripheries:

| 0 | 1 | 1 | 1 | 0 |
| 1 | 2 | 2 | 2 | 1 |
| 2 | 2 | 2 | 2 | 2 |
| 1 | 2 | 2 | 2 | 1 |
| 0 | 1 | 1 | 1 | 0 |

But I think I could probably do so at a later time.

I actually have a question now - do phosphenes flicker if you simulate them
continuously? Do they adapt? It would seem that these would be quite important
to know, but I don't think I've come up against it yet in my (extremely early)
literature reading...I suppose I should keep an eye out.

** <2019-03-11 Mon>
:LOGBOOK:
CLOCK: [2019-03-11 Mon 14:15]--[2019-03-11 Mon 16:43] =>  2:28
CLOCK: [2019-03-11 Mon 09:26]--[2019-03-11 Mon 12:39] =>  3:13
:END:

Briefly saw Yan - he says to be more realistic about the number of phosphenes
available in the map, and maybe add more noise to see how that changes
perception. It was 64 * 48 so I've reduced it down to about 13 * 10 (he said
around 100 phosphenes would be state of the art).

I've created a small dataset of 200 images to see if I can try to classify my
keys...I've put a  label around the keys for each for the 200 images.

I'm using this resource to look at object detection with rectangles:

https://www.mathworks.com/help/vision/examples/object-detection-using-faster-r-cnn-deep-learning.html


There's a function that converts groundTruth data to trainingData. It's so easy
to just string up a neural network in Matlab, I kinda of feel like it's
shamefully easy...

Training takes a while. 1300 iterations (epoch 7) using the network described by
Mathworks takes 10 minutes.

It's surprising though - the mini-batch accuracy gets up to aroud 90%. Hopefully
it can generalise well enough...

(In fact, it seems on Step 2, it reaches 100% on some times. I wonder if that's
just overfitting).

The detector trained and I saved it - and it works! Actually not shabby at all,
considering the model was completely naive (as well as the parameters I used)
and I only used 200 images (which I took in a stream) from my webcam.

The latency is very noticeable unfortunately, but then again, I wonder whether
that would be a problem for this use case - for example, if you're looking for
something and you're not seeing the environment, would it be so bad? You don't
see yourself and you don't see the whole environment spinning around you - just
one thing on the screen (or nothing). Could be a quick screen.

Anyway current issues:

- Won't detect things unless same size as training data (is there a solution for
  this?) - wait actually it can cope with some different sizes (but not all)j
- Colour matters

-----

I spent about 2 hours today labelling ground truth pixels for my backpack...

-----

And I ended up abandoning the image segmentor. I realised I had to segment
/everything/ in the image, and I just am not prepared to do that at the moment. I
stashed it away. I think object detection makes more sense at this stage - after
all, I'm just asking to detect objects.

** <2019-03-13 Wed>
:LOGBOOK:
CLOCK: [2019-03-13 Wed 10:22]--[2019-03-13 Wed 14:11] =>  3:49
:END:

I might try to look into doing the whole "evaluating classification accuracy"
for different image processing algorithms - I think it could actually be a nice
quantitative way of measuring it (and also I can make this a "standard" way of
measuring the classification accuracy for a new image processing algorithmI
define. )


Some datasets to look at :

1. CIFAR-10

   http://www.cs.utoronto.ca/%7Ekriz/cifar.html

2. Caltech 101

   http://www.vision.caltech.edu/Image_Datasets/Caltech101/

3. MNIST Database

   http://yann.lecun.com/exdb/mnist/

*** Meeting Notes :meeting:
**** Status
   1. Have a plan now for discussion (hopefully)
   2. Started toying with different image processing implementations
   3. Continuing to read through literature in prep for lit review
   4. Made a project timeline

   For discussion in particular today:

   *Project plan + project timeline*

**** Project Plan

***** Context

   1. Visual pathways can be damaged and vision can be lost.
   2. Stimulating visual cortex -> phosphenes => regain vision?
   3. BUT
      1. Irregular phosphene maps
      2. Non-dependenable intensity levels/colour
      1. Very low resolution (stats?)

***** Big Question

   /How can we make an irregular, binary, low-resolution representation of vision
   most useful?/

   (and by extension, how can we make an intracortical visual prosthesis most
   useful?)

   #+begin_src ditaa :file ./presentations/ditaa/w2_intro.png :exports results
                       +-----+
                       |     |
       Camera Image -> |  ?  | -> Phosphene Map -> Electrodes -> ...
                       +-----+
   #+end_src

   #+RESULTS:
   [[file:./presentations/ditaa/w2_intro.png]]

***** Possibilities

****** Direct

   Intensity -> binarise -> downsample

****** Filters

   - Gaussian / other blur
   - Difference of Gaussians / Laplacian / other edge-detection

****** "Special" phosphenes

   - Depth-encoding phosphene (Mohammadi 2012)

****** Selective

   - "Empty ground" (Lui 2012)

****** Transformative

   - Face -> avatar (Lui 2012)

****** Augmentation with other modalities

   - Auditory?
   - Haptic?

***** Issues

   - Direct ::
	Dramatic information loss inevitable -> indistinguishable.

   - Filters ::
	Dramatic information loss inevitable -> indistuinguishable.
	Edges lost (too low-resolution).

   - "Special" phosphenes ::
	Phosphene fidelity questionable.

   - Selective ::
	Loss of general information. Also, how?

   - Transformative ::
	Transformations need to be low-resolution. Distinguishable?
	Features may not be distinguishable.

   - Augmentation with other modalities ::
	Requires additional hardware. Useful? Confusing?

***** Proposed Focus: User-Directed Selective Sight

   Create a set of trained models.
   User asks for object in trained models -> phosphenes highlight bbox.

   1. Image expectation guided by user -> reduce information possibility space
      Spatial information providable (where is it roughly?)
      Information becomes binary (is it present or not?) if very low-fidelity.

   2. Computer vision and software capabilities progressing rapidly.
      Pure-image object recognition becoming a possibility.

   3. Does not have to be the only "mode".
      (Can simply supplement other techniques).

   (Possibility: interacting with simple audio adjunct?)
   (Interfacing with user for selection?)

***** Demo: Recognising My Keys

   Quick-and-dirty demo (clearly not polished...):
     - Training data from 200 frames of me dangling my keys in front of my webcam
       in a single video take (about 3-5 seconds of video)
     - Quickly labelled with bounding boxes
     - "Out-of-the-box" R-CNN (2x covol, 3x RELU, 1x pooling, 2x fully connected)

   (Show Keys Demo)

   Hopefully shows vague (but potentially useful) positional sense even on lossy,
   irregular map.

   Note - some classification error (although the network is very rudimentary and
   training data very small/unsanitised...)

***** Unknowns

   1. Is this useful?
      (Specific scenarios? e.g. looking at a table?)
   2. Is latency reducible?
   3. Is classification accuracy acceptable?
   4. Is this safe?
   5. Is training time acceptable/training process establishable?
   6. How necessary is it? (Would learning/neuroplasticity eventually be able to derive
      useful information from simple methods?)

***** Possible Testing Framework and Plan

   0. (?Survey on areas of difficulty for visually impaired)
   1. Develop model training process to quickly make training data
   2. Train models and investigate model parameters for useable networks
   3. Work with Psychophysics toolbox to develop testing experiments against
      controls (intensity, edge-detection) e.g.:
      1. Object arrangement task?
      2. Situational task?
   4. Run experiments + gather qualitative data
   5. Analyse data (statistical difference between object arrangement
      accuracy/time)
   6. Writeup?

***** Proposed Hypothesis

   User-directed and deep-learning-based selective phosphene rendering improves
   user performance on object manipulation and situational tasks in simulated
   prosthetic vision.

***** Meta Issues

   1. What's actually novel about the project?
      (Maybe only the application to SPV part? Is that even novel?)

   2. How translatable is it to an intracortical visual prosthesis?
      (Hardware limitations? Memory limitations? Training limitations?)

   3. How convincing would the Psychophysics results be?
      (How many participants required? Any other methods of measuring?)

   4. How feasible is the project?
      (Timeline to help guide and reassess progress where necessary?)

**** Timeline :timeline:

***** March

   #+begin_quote
   [2019-03-29 Fri] - Finalise project title.
   #+end_quote

   - Decide on testable hypothesis for honours project
   - Begin prototyping image processing methods + training methods
   - Begin interfacing with Psychophysics toolbox for experiment design
   - Get to "halfway" mark on literature review (not necessarily writing, but
     collected references and writing strucure)

***** April

   #+begin_quote
   [2019-04-17 Wed] - Progress report due - discussed and signed by supervisor.
   #+end_quote

   - Finish literature review draft, for review.
   - Conduct survey(?) to identify areas of most benefit for visually impaired
     patients
   - Decide specifics of experimental design (what objects, what models) + collect
     and label data for training

***** May

   #+begin_quote
   [2019-05-01 Wed] - Department Oral No. 1.

   [2019-05-15 Wed] - Literature Review due.
   #+end_quote

   - Finish and submit literature review.
   - Investigate and iterate network parameters and Psychophysics task, + reassess
   - Recruit participants for experiment.

***** June

   - Investigate methods of improving object recognition + reassess
   - Finalise experimental design and trial run experiment
   - If time: work on adjuncts (e.g. auditory feedback)

***** July

   - Conduct experiment and reiterate if necessary
   - Conduct preliminary analysis of results
   - If time: work on adjuncts (e.g. auditory feedback)

***** August

   - Conduct experiment and reiterate if necessary
   - Analyse results
   - Work on thesis

***** September

   #+begin_quote
   [2019-09-25 Wed] - Department Oral No. 2
   #+end_quote

   - Work on thesis
   - Complete draft thesis by end of September, for review

***** October

   #+begin_quote
   [2019-10-29 Tue] - Thesis Due

   [2019-10-29 Tue] - Faculty Oral and Poster upload to Moodle Due

   [2019-10-30 Wed] and [2019-10-31 Thu] - Faculty Oral and Poster Days
   #+end_quote

   - Refine thesis
   - Complete and submit project
**** Post-Meeting Notes

   1. Temporal resolution to be considered in simulated prosthetic vision
      - May be difficult for actual VR considering it makes people sick, but at
	least something to keep in the back of the mind
      - Do phosphenes decay? At what rate? Important consideration for refresh
	rate.
   2. Are there ways of quantifying the information loss from different image
      processing methods?
      - e.g. Landolt C - after simulated image processing, is it still classifiable
	by a computer, then by a human? Compare different image processing methods.
   3. Are there ways of quantifying the best image processing method for a given
      phosphene map?
      - e.g. Given a specific phosphene map (possibly largely variable positioning,
	intensity and size), can we determine "best" image processing algorithm
	(from a given subset?)
   4. Need to consider what is an /actual/ area of need for visually impaired
      community
      - Already have a lot of capacity (perhaps moreso than is usually thought) -
	what can a bionic eye actually /add/? And is the additional capability
	worth it?
      - Identify an area of actual need - do a review of literature surrounding
	this.
   5. [ ] Confirm - who organises Department Oral? Also timing - is it a hard deadline?
   p

** <2019-03-15 Fri>
:LOGBOOK:
CLOCK: [2019-03-15 Fri 15:09]--[2019-03-15 Fri 15:13] =>  0:04
:END:

Spent a bit of time today trying to get a processor and renderer so I can
eventually make a pipeline of image -> processor -> renderer -> classifier.

I think it might be good to think of the processor as producing a set of
phosphene modulation indices (but binary only!) and the renderer as simply
taking those indices and mapping it to a simulated image.

** <2019-03-17 Sun>
:LOGBOOK:
CLOCK: [2019-03-17 Sun 11:45]--[2019-03-17 Sun 20:00] =>  8:15
:END:

I'm going to try doing a little work today on getting the "evaluation" step of
processing done. First I need to find some images of Landolt Cs or similar...

I might just use the MNIST dataset for now, it's easier.

I downloaded the MNIST data from

http://yann.lecun.com/exdb/mnist/


Ahh, but it seems like there might be an easier data format to use from
Kaggle...

This blog mentions is:

https://blogs.mathworks.com/loren/2015/08/04/artificial-neural-networks-for-beginners/#1168dbb4-1365-4b63-8326-140263e2072f

And the data is available from:

https://www.kaggle.com/c/digit-recognizer/data

I used this data (csv, easy to read) and preprocessed it to training and test
images. It's a pity they don't provide labels for the test set, so I just
divided the training set using a 70/30 split.


-----

I made a little function to show the original, processed and rendered MNIST
image. I think it's good to visualise these things.


-----

I just found out Matlab comes with some pretrained classifiers...That might be
useful to use in the future for some quick demo type stuff...

https://www.mathworks.com/help/vision/examples/image-category-classification-using-deep-learning.html

-----

I tried using fitcecoc on the the full (70%) training data set (internal), but I
was running it for about half an hour and it didn't stop. I tried on 100 rows
and it ran almost instantly. I guess I'll just train it on a subset then...

I've tried again on 2940 images (10% of the dataset) and it's still taking a
while. About 5 minutes so far...

But it did eventually finish! Maybe about 5-10 minutes?


-----

Okay, now I've got four processors - intensity, edges, and two "transformative"
ones specially for MNIST. I think I need to think about how exactly to make
transformative renderers for the more general case, but this should be okay for
now I think...

So since I've got four processors and two renderers, I should be able to start
making some training and test images for classification.


-----

So I've trained on 1000 images and made models for each process/render/size, and
now am going to try classifying 500 images. The number of each is fairly
arbitrary - the model training was actually very fast this time, despite the
images being larger (maybe because they're binary?) and completed in less than
10 minutes for all of them (though after that is when my computer froze). 

-----

But actually, it just hit me that there's an upper limit on the accuracy of
mnistBraille and mnistMimic - the accuracy of the original SVM...which they
might be hitting, since I only trained the original SVM on 10% of the full
dataset...

The flow is basically:

1. renderAndSave (training)
2. trainOnSavedRenders
3. renderAndSave (test)
4. evaluateAccuracy

Okay, after re-running, I think the results are a lot more consistent. I've also
added the control, and low and behold, it's a 91% accuracy (the same as all the
mnist transformation ones).

Now that I've run it though, it seems like the result is self-evident...of
course, a transformative approach is going to achieve almost 100% accuracy.
Maybe the irregular map isn't irregular enough. The model was definitely able to
learn from the positions of the phosphenes on the irregular map it seems...

So it seems that the transformative method doesn't lose any information even at
3x3 irregular maps (all the information loss is in the initial SVM
classification step).

Todos for tomorrow:

1. Graph accuracy results
2. Make montages
3. Write into something cohesive for weekly

Also maybe put graphics in the writing directory version control after all...
** <2019-03-18 Mon>
:LOGBOOK:
CLOCK: [2019-03-18 Mon 11:28]--[2019-03-18 Mon 15:28] =>  4:00
CLOCK: [2019-03-18 Mon 11:27]--[2019-03-18 Mon 11:28] =>  0:01
:END:

I think today I'll work on presenting the results and I might try running an
"irregularChanging" renderer as well just for comparison's sake....

I think I'll present the final accuracy results in Julia (though the montages
are probably easier to do in Matlab)...

-----

Okay, I've rerun it for the irregular changing map...now to see if it makes a
difference. I expect that the classification accuracy for all of them for the
irregular changing map should be significantly lower....

-----

Plotting in Julia and working with the notebook is...not as good as I thought it
would be. Running a cell is /slow/ for the first time, which is very
frustrating.

Also, plotting is a little harder than it needs to be.  I can't specify the
order of inidividual bars using groupedbars from StatsPlots...let's see if I can
with gadfly...

Okay, Gadfly makes it a  little easier. I really like Gadfly now...

-----

Well, anyway, I managed to get the plots done and the montages done, and even
ran another renderer (irregularChanging). 

** <2019-03-19 Tue>
:LOGBOOK:
CLOCK: [2019-03-19 Tue 12:16]--[2019-03-19 Tue 14:16] =>  2:00
CLOCK: [2019-03-19 Tue 09:49]--[2019-03-19 Tue 12:13] =>  2:24
:END:

I thought yesterday about whether I should try and do some real-time processing
with the pretrained neural network, but since it's not an R-CNN, I don't think
it'd quite do what I want...and there's still a lot of things I need to do for
my literature review and general understanding of neurophysiology, so I think
today I'll just work on literature review and cementing my own understanding.


-----

Wrote up my weekly for tomorrow. I think I'll just take it a bit slower today,
just work on understanding things and connecting ideas...and my literature
review and direction.

** <2019-03-20 Wed>
*** Meeting Notes :meeting:

**** Status

   - Quick-and-dirty mix-and-match (of processors + renderers) results
   - Continuing literature review
   - Who organises department oral?

     #+begin_quote

     This presentation can be held any time prior to the deadline stipulated in the
     Unit Guide, Course Guide and Moodle [March 1st]. Each School will advise
     Supervisors and Students whether the School, Department, or Supervisor are
     responsible for arranging the date and venue for the Oral Presentation and, if
     the supervisor or Department arranges the seminar, to whom the grading sheets
     should be returned to.

     #+end_quote

**** "Mix and Match"

***** Quantifying "Information Loss" From Processing Methods

   #+begin_src ditaa :file ./presentations/ditaa/w3_intro.png :exports results

    +-------+ Process +---------+ Render +----------+
    | Image |-------->|Processed|------->|Phosphenes|
    +-------+         +---------+        +----------+
                    (For rendering)
   #+end_src

   #+RESULTS:
   [[file:./presentations/ditaa/w3_intro.png]]

   Some questions from last week:

   1. Is there /actually/ any information lost for particular processing methods?
   2. Can we compare how much is lost?
   3. Does the information lost depend on rendering method?

   Last week, we discussed using *classification accuracy* on the rendered
   phosphenes to determine if the original image remains classifiable.

***** Methods

   #+begin_src ditaa :file ./presentations/ditaa/w3_methods.png :exports results

                            +-------------+
                            |MNIST DATASET|
                            +-------------+
                                   |
                                   V
   +----------------------------------------------------------------+
   |                                                                |

    +---------+   +--------------+   +-------------+  +------------+
    |Intensity|   |Edge-Detection|   |MNIST-Braille|  | MNIST-Mimic|    (Processors)
    +---------+   +--------------+   +-------------+  +------------+

                                    X

       +--------+   +-------------------+  +-------------------+
       | Regular|   |Irregular(Constant)|  |Irregular(Changing)|        (Renderers)
       +--------+   +-------------------+  +-------------------+

                                   X

	+-----+           +-----+               +-------+
	| 3x3 |           | 5x5 |               | 10x10 |               (Sizes)
	+-----+           +-----+               +-------+

   |                                                               |
   +---------------------------------------------------------------+
                                    V (FOR EACH COMBO)
                    +------------------------------+
                    |      SIMULATED RENDERS       |
              +-----|-----(2000 for training)      |
              |     |     (+ 500 for testing)------|--------+
              |     +------------------------------+        |
              |                     |                       |
              |                     V                       |
              |   +------------------------------------+    |
              +-->|TRAINED MULTI-CLASS DECODERS (2000) |    |
		  |(ECOC, 45 binary SVMs on flattened) |    |
		  +------------------------------------+    |
                                    |                       |
                                    V                       |
		       +------------------------+           |
		       | TESTED DECODERS (500)  |<----------+
		       |   ACCURACY ASSESSED    |
		       +------------------------+



   #+end_src

   #+RESULTS:
   [[file:./presentations/ditaa/w3_methods.png]]

****** Notes

   - MNIST Dataset :: Sourced from [[https://www.kaggle.com/c/digit-recognizer/data][public Kaggle dataset]] (as it was a CSV and quick
	to process)...
     - Why MNIST? Couldn't find Landolt C dataset (though it could be generated)
       and wanted to test on something "real". MNIST is also clean and well-tested.
   - MNIST-Braille and MNIST-Mimic :: Recognises MNIST digit using SVM (trained on
	1000 images to save on time on my laptop...) then:
     - MNIST-Braille :: Converts recognised digit to clearn 2x2 Braille representation then upscales
	  to grid size.
     - MNIST-Mimic :: Converts recognised digit to clean 3x3 or 5x5 digit-like
	  representation, then upscales to grid size.
   - Renders :: All renders map to a grid of circular phosphenes (convoluted with 2D
	Gaussian kernel to simulate feathering), with the final rendered image
	being 56x56 pixels (source was 28x28).
     - Irregular Renders :: Render grid was transformed to 2D polar coordinates and
	  noised such that theta and r were altered by factor in range (0.95, 1.05)
	  and intensity altered by factor in range (0, 1). I transformed to polar
	  coordinates to (in the future, maybe) deform phosphenes based on radius
	  (foveal area corresponds to smaller phosphenes?)

***** Visualising Mix and Match: An Example

****** Original MNIST Image

   [[./02-mix-and-match/graphics/mixAndMatch_original.png]]

****** Render Montages

   FORMAT:

   |---------------+-----------+----------------+------------------+--------------------|
   | METHOD        | PROCESSED | REGULAR RENDER | IRREGULAR RENDER | IRREGULAR RENDER 2 |
   |---------------+-----------+----------------+------------------+--------------------|
   | INTENSITY     |           |                |                  |                    |
   |---------------+-----------+----------------+------------------+--------------------|
   | EDGES         |           |                |                  |                    |
   |---------------+-----------+----------------+------------------+--------------------|
   | MNIST-BRAILLE |           |                |                  |                    |
   |---------------+-----------+----------------+------------------+--------------------|
   | MNIST-MIMIC   |           |                |                  |                    |
   |---------------+-----------+----------------+------------------+--------------------|

   NOTE: =IRREGULAR RENDER 2= corresponds to the "Irregular Changing" method, but
   as this is only a snapshot of a single MNIST digit, this doesn't really mean
   anything...so it's effectively just another irregular render.

******* 10x10 Phosphene Grid

   [[./02-mix-and-match/graphics/mixAndMatch_s10.png]]

******* 5x5 Phosphene Grid

   [[./02-mix-and-match/graphics/mixAndMatch_s5.png]]

******* 3x3 Phosphene Grid

   [[./02-mix-and-match/graphics/mixAndMatch_s3.png]]
******* Observations

   1. Intensity and edge-based methods are liable to complete failure at very low
      resolution (particularly for a thin figure like the 4)
   2. Source images (MNIST dataset) are NOT bounding-boxed (potentially significant
      as the transformative methods /do/ effectively upscale the
      representation...but even if they were bounding boxes, could be argued that
      the digit handwritten forms would still NOT necessarily translate cleanly
      into a 3x3 form.)
***** Results

****** Bar Graphs of Classification Accuracy

******* 10x10 Phosphene Grid

   [[./02-mix-and-match/graphics/mixAndMatch_results10.png]]

******* 5x5 Phosphene Grid

   [[./02-mix-and-match/graphics/mixAndMatch_results5.png]]

******* 3x3 Phosphene Grid

   [[./02-mix-and-match/graphics/mixAndMatch_results3.png]]

***** Discussion

   1. A /constant/ irregular phosphene map is not really an issue for a machine
      decoder (though a /changing/ irregular map does lower performance)
   2. The Mnist-Braille and Mnist-Mimic processors achieve parity with the SVM they
      use, which implies that /most of the information is lost in the processing
      step/ (for a machine decoder)
   3. Trasformative methods are robust against increasingly low-resolution sizes
      (not unexpected given they transform the data into explicitly unique
      encodings), /but this is only as good as the original recognition accuracy/
      - Implication - can machines achieve better recognition accuracy than humans?
	I would argue, since the machine has access to the full-resolution image
	(whereas the human only has access to the downsampled, binary image),
	/yes/ - maybe machines are worse than humans if they had the same source
	image, but they /don't/ have the same source image to work with.
      - Therefore, "pre-recognising" the image according to user's will, to
	constrain the meaning of phosphene encodings, may be useful. (I mean, if
	you only had a 3x3 grid, you can only differentiate 2^9 = 512 unique states
	of the world with your "general" vision ).
    4. How would this translate to human recognition?
       - Would an irregular (constant) map be more of a deterrent? Maybe not.
       - How long would it take to learn how to decode representations?
       - Would the additional information provided by a moving stream of images be
	 enough to supplement intensity/edges to accurately decode?

***** Summary

   1. "Smart" specific transformative processing methods provide greater machine classification
      accuracies than "general" processing methods and are robust even at very low resolutions.
   2. These methods require specific training and use-case targeting, but can
      provide clean (+ redundant to defend against low fidelity) encodings with
      constrained meaning (which general methods lack).
   3. Unknown how this would translate to real-time image stream in humans.
      Potentially a point to test).

**** Post-Meeting Notes

   - Landolt C / Tumbling Es may be able to be generated from within Matlab - to
     investigate and use (check Psychtoolbox)
   - Brindley & Dobelle & Schmidt - may be able to map electrode locations to
     actual phosphene locations with data from these papers (maybe...) - to
     investigate (and think about making actual phsphene locations more based on
     real data)
   - Image statistics - may be clasifying images into categories based on
     statistics (e.g. spatial frequencies? to characterise "thinness") and grouping
     images based on these statistics for determining "good" and "bad" images for
     different classification methods
   - Classify "like a human" - handicapping the machine classifier (e.g. by adding
     random noise) until classification reaches human-level classification; then
     the handicapped machine classifier could be a reusable approximation of a
     human learner
   - Think about classifying different types of images (e.g. Landolt C/Tumbling Es,
     natural images) and different methods/use-cases (e.g. object detection,
     navigation, whole-image classification, localisation) - maybe choose just one
     or two to focus on on one-or-two types of images
   - Augmented reality research - might be a good source of literature to think
     about how supplemental information could be provided (e.g. haptic, auditory)
     other than just purely visual information
**** Plan for This Week

   1. Continue working on literature review
   2. Continue tinkering

** <2019-03-22 Fri>
:LOGBOOK:
CLOCK: [2019-03-22 Fri 12:22]--[2019-03-22 Fri 13:22] =>  1:00
:END:

I think I have to spend some time just going through the literature and
methodically identifying the gaps I see at the moment...then trying to
brainstorm how to approach these gaps.

** <2019-03-26 Tue>
:LOGBOOK:
CLOCK: [2019-03-27 Wed 00:16]--[2019-03-27 Wed 01:16] =>  1:00
CLOCK: [2019-03-27 Wed 00:14]--[2019-03-27 Wed 00:16] =>  0:02
CLOCK: [2019-03-27 Wed 00:09]--[2019-03-27 Wed 00:14] =>  0:05
CLOCK: [2019-03-26 Tue 09:39]--[2019-03-27 Wed 12:39] =>  3:00
:END:

It's been really hectic recently trying to balance my three part-time
jobs, CIGMAH and honours...

I think today, I just need to get an iron clad argument for my lit review set up
and make sure I have something to talk about for the weekly tomorrow...

** <2019-03-27 Wed>
*** Meeting Notes :meeting:
**** Status

   - Continuing to work on literature review...starting to get a move on.
   - Repeated last week's "experiment" with Landolt Cs

**** Gaps

   #+begin_src ditaa :file ./presentations/ditaa/w4_questionflow.png :exports results

	  +-----+
     +--> |Scene|
     |    +-----+
     |       |
     |       | <------------------ What scenes?                             (Brady 2013)
     |       | <------------------ How often?                               (?)
     |       |
     |       |
     |       v
     |    +------------+
     |    |Camera Image|
     |    +------------+
     |       |
     |       | <------------------ What is most useful?                     (?)
     |       | <------------------ What is achievable in real-time?         (?)
     |       | <------------------ What has been used before?               (Reviewed Chen 2009)
     |       |
     |       v
     |    +---------------+
     |    |Processed Image|
     |    +---------------+
     |       |
     |       | <------------------ Can phosphenes be mapped?                (Dobelle 1974, Schmidt 1996, Kaskhedikar 2017)
     |       | <------------------ Is phosphene mapping necessary?          (?)
     |       | <------------------ How much information per phosphene?      (Dobelle 1974)
     |       |
     |       |
     |       |
     |       |
     |       v
     |    +-----------------+
     |    |Per-electrode PMI|
     |    +-----------------+
     |       |
     |       | <------------------ How reliable are phosphene locations?    (Dobelle 1974, ...)
     |       | <------------------ How large are phosphenes per electrode?  (Dobelle 1974, ...)
     |       |
     |       |
     |       v
     |    +--------------+
     |    |Rendered Image|
     |    +--------------+
     |       |
     |       | <------------------ How long does rendered image last?       (Dobelle 1974, ...)
     |       | <------------------ Is this image useful?                    (Brady 2013)
     |       |
     |       |
     +-------+



   #+end_src

   #+RESULTS:
   [[file:./presentations/ditaa/w4_questionflow.png]]

**** Tasks + Focus

   1. Identification (Brady 2013)
   2. Reading (Brady 2013)
   3. Description (Brady 2013)
   4. Navigation (Giudice 2008)
   5. Localisation (e.g. Weeraratne 2012)

   Potential focus: Natural scene object localisation (discussed previously).

**** Question (maybe off the rails...)

   Is the approach of simulating prosthetic vision as /vision/ necessary?

   Corollary: is phosphene mapping necessary?

   (i.e. maybe reframing as an /information mapping/ problem (from states of world
   -> stream of discrete bits) would be a very interesting perspective...possible
   augmentation outside of constrains of "having to transform to a recognisable
   image")

**** "Mix-And-Match" Run 2 - Landolt Cs

   Same as last week, but repeated for LandoltC.

   Reasoning:

   - Input data is bounding boxed, not center of gravity.
   - Simpler input image data -> more "clean" evaluation of information loss.
   - Less classes, less decoders, shorter to run.

***** Methods (Similar to Last Week)

   #+begin_src ditaa :file ./presentations/ditaa/w4_mixmethods.png :exports results

                            +-------------+
                            | Landolt Cs  | (Made in Blender)
                            +-------------+
                                   |
                                   V
   +----------------------------------------------------------------+
   |                                                                |

       +---------+   +--------------+   +---------------+
       |Intensity|   |Edge-Detection|   |Landolt C Mimic|               (Processors)
       +---------+   +--------------+   +---------------+

                                    X

       +--------+   +-------------------+  +-------------------+
       | Regular|   |Irregular(Constant)|  |Irregular(Changing)|        (Renderers)
       +--------+   +-------------------+  +-------------------+

                                   X

	+-----+           +-----+               +-------+
	| 3x3 |           | 5x5 |               | 10x10 |               (Sizes)
	+-----+           +-----+               +-------+

   |                                                               |
   +---------------------------------------------------------------+
                                    V (FOR EACH COMBO)
                    +------------------------------+
                    |      SIMULATED RENDERS       |
              +-----|------(200 for training)      |
              |     |     (+ 500 for testing)------|--------+
              |     +------------------------------+        |
              |                     |                       |
              |                     V                       |
              |   +------------------------------------+    |
              +-->|TRAINED MULTI-CLASS DECODERS (200)  |    |
		  |(ECOC, binary SVMs on flattened)    |    |
		  +------------------------------------+    |
                                    |                       |
                                    V                       |
                       +------------------------+           |
                       | TESTED DECODERS (500)  |<----------+
                       |   ACCURACY ASSESSED    |
                       +------------------------+



   #+end_src

   #+RESULTS:
   [[file:./presentations/ditaa/w4_mixmethods.png]]

***** Visuals

****** Original

   [[./02-mix-and-match/graphics/mixAndMatch_landoltc_original.png]]

****** 10x10 grid

   [[./02-mix-and-match/graphics/mixAndMatch_landoltc_s10.png]]

****** 5x5 grid

   [[./02-mix-and-match/graphics/mixAndMatch_landoltc_s5.png]]

****** 3x3 grid

   [[./02-mix-and-match/graphics/mixAndMatch_landoltc_s3.png]]

***** Results
****** 10x10 grid

   [[./02-mix-and-match/graphics/mixAndMatch_landoltc_results10.png]]

****** 5x5 grid

   [[./02-mix-and-match/graphics/mixAndMatch_landoltc_results5.png]]

****** 3x3 grid

   [[./02-mix-and-match/graphics/mixAndMatch_landoltc_results3.png]]
***** Discussion

   - Similar results to last week, but cleaner comparison (original actually
     reaches 100% classification accuracy, as it should...)
   - Transformative method robust against different grid sizes, direct methods not
     so.
   - Transformative method also fairly robust against an irregular, changing
     phosphene map due to redundancy.
**** Post-Meeting Notes

   - Hard deadlines:
     - LIT REVIEW DRAFT in 2 WEEKS
     - DEFINITIVE PLAN/FOCUS by 1 MONTH
   - READ up on a Primer on the Visual System
     - SfN
     - Kandel
   - Constrain the demographic to people who have lost vision /recently/, may be
     more manageable problem
   - Localisation and information (what) should be considered together
   - Start thinking about putting some slides together and practising presentation
     skills.
   - Working with MNIST is still an option for "semi-real" situations.
   - Think about using available Landolt C psychophysics data (Chen, Hallum)

* April 2019

   #+begin_quote
   [2019-04-17 Wed] - Progress report due - discussed and signed by supervisor.
   #+end_quote

#+BEGIN_QUOTE
[2019-06-11 Tue 13:29] Note: this month is particularly incomplete,
mostly as I was working on my oral presentation and literature review
and doing all the writing there.
#+END_QUOTE

** <2019-04-02 Tue>

*** Meeting Notes :meeting:
**** Status

   Deadlines and timelines:

   #+begin_quote
   [2019-04-17 Wed] - Progress report due - discussed and signed by supervisor.
   #+end_quote

   - [ ] Final confirmation of project title.
   - [ ] Progress report - will bring for discussion next week.
   - [ ] Will confirm Department Oral arrangements if no further information by Friday.
   - [ ] /I will have a literature review draft by next Wednesday, *hard deadline*./
     It may be rubbish, but I will have one.

**** Literature Review

   - Focus ::  I'd like to focus on computer vision-aided (simulated) prosthetic vision as an "overarching"
	topic for the literature review.

     - Key questions:

       1. What is the current state of computer vision assistive technologies
	  for the visually impaired? (not necessarily prosthetic)
       2. What is the current state of image processing for visual prosthetics?
       3. What (or are there even any) deficits are there in current image
	  processing methods for visual prosthetics and can past research on
	  computer vision assistive technology be applied to visual prosthetics?
       4. What are the unknowns in translating computer-vision-processed
	  information into visual-prosthetic-information? (essentially the
	  interpreting non-visual information part - how far can this be taken?
	  What's known about it? )

   - Rationale :: I hope this reconciles a few different parts which I'd like to
	focus more closely on.

	#+begin_src ditaa :file ./presentations/ditaa/w5_overview.png :exports results



                           "State of the World" Information

                                             |
                                             v

                             Functional Context Information  ------------------------------+
				  /            |       \                                   |
                              /---             |        -----\                             |
	 Direct Approach  /---                 |              ------\                      |
	 +----------------+-------------+      |                     ----                  |
	 |      Visual Stream           |   Audio Stream           Other (Tactile etc.)    |
	 |                              |                                                  | User-Guided
	 |         /                    |                                                  |
	 |        /   Capture           |                                                  |
	 |                              |   Computer-Vision Augmented Approach             |
	 |   Camera Image --------      |  +-----------------------------------------+     |
	 |                        \------> |Preliminary Information Inference  <-----+-----+
	 |       |                      |  |                                         |     |
	 |       |    Process           |  |       |     Process                     |     |
	 |       v                      |  |       v                                 |     |
	 |                              |  |                                         |     |
	 | Phosphene Modulation Indices |  |Phosphene Modulation Indices             |     |
	 |                              |  |                                         |     |
	 |       |                      |  |       |                                 |     |
	 |       |    Stimulate         |  |       |     Stimulate                   |     |
	 |       |                      |  |       |                                 |     |
	 |       v                      |  |       v                                 |     |
	 |                              |  |                                         |     |
	 |   Rendered Image             |  |Rendered Image                           |     |
	 |                              |  |                                         |     |
	 |       |                      |  |     |                                   |     |
	 |       |    Stream            |  |     |    Stream                         |     |
	 |       v                      |  |     v                                   |     |
	 |                              |  |                                         |     |
	 |   Information Inference      |  |Information Inference   <----------------+-----+
	 +------------------------------+  +-----------------------------------------+



	#+end_src

	#+RESULTS:
	[[file:./presentations/ditaa/w5_overview.png]]

     1. Cortical visual prosthetics (currently) produce very low-resolution information
	(with low fidelity, irregular spatial layout, though these are assumed
	to be be constant once implanted.)
     2. The /additional/ utility of such information /beyond/ that currently
	available to the visually impaired population needs to be justified for
	an invasive procedure.
     3. There are /inherent/ representation limits posed by very low-resolution
	information, which means any information presented /must/ be
	context-dependent to extends inherent limits.
     4. Context-dependency based on retained /natural/ senses (e.g. auditory,
	tactile) are useful in some scenarios (e.g. talking face-to-face,
	movement guided by proprioception), but infeasible in other scenarios
	(e.g. object recognition, crowd-searching, reading navigation text)
	which arguably are the /greatest/ current gap in capacity for the
	visually impaired (i.e. where current workarounds or technology are
	lacking).
     5. However, context-dependency based on computer-interpreted images or
	other inputs provides /additional/ information not currently afforded by
	retained natural senses, and the possible "interpretable input"
	resolution is far greater (that of the direct camera image, vs that of
	the phosphene map).
     6. Computer vision has been applied to many domains, including that of
	assistive technology for the visually impaired. However, there are
	limited cases where this has been applied to cortical visual
	prosthetics.
     7. There remain questions on:

	1. How current computer vision technology has been applied, and with
           what success, to assitive technology for the visually impaired.
	2. How current image processing techniques for prosthetic vision fare in
           terms of daily utility.
	3. How people would cope with non-direct mappings of scene information
           (and possibly non-scene information) to the visual system (perhaps this
           could exist on a graded scale, from directly mapping image to
           phosphenes, to "similar" with only minor alterations, to completely
           dissimilar).
           - Example (crazy) experiment: substituting humans for the last layer of a CNN
             and optimising weights for /human/ classifier (so the network learns
             remapped information representation which optimises for human
             classification at the end). Would need a streamlined process and a
             /lot/ of data, but would be interesting experiment between
             psychophysics and computer science...
           - I'm wondering whether there are problems with putting abstracted
             information at the level of V1, when it usually deals with lower-level
             features. Don't know how to test that in simulated prosthetic vision though...
	4. What context-dependent mappings would be feasible (the idea of modes -
           e.g. reading mode, "finding" mode, navigation mode) and useful.

   - Project title :: Used to be (way back last year)
	"Exploring methods of conveying visual information through phosphenes
	produced from a cortical visual prosthesis", but maybe change to
	"The utility of computer vision augmentation of simulated
	prosthetic vision." (or along those lines) for more specificity.

     - Outline ::

       1. Current image processing methods for simulated prosthetic vision

	  - Chen 2009 (review)
	  - Lui 2011 (transformative)
	  - Mohammadi 2012 (special-encoding phosphenes)
	  - Abolfotuh H 2016
	  - Vergnieux 2017 (computer vision-detected distance and wireframe mapping
            for navigation)
	  - Guo 2018 (computer vision-aided for object saliency in object detection)

            Lot of interesting stuff on retinal side:

	  - Li H 2017 (real-time saliency detection and enhancement)
	  - Cao X 2017 (correlating task accuracy with phosphene irregularity
            distortion and droupout)
	  - Li H 2018 (computer-vision guided object detection and improving object
            saliency - a bit like the keys demo)

       2. Matching suitability of current image processing methods with areas of
	  need and identifying deficits

	  - Brady 2013 (visual challenges in everyday life)
	  - Giudice 2008 (blind navigation)
	  - Weeraratne 2016 (challenges for medication usage)

       3. Current uses of computer vision techniques for assistive technology for
	  visually impaired and potential applications to prosthetic vision.

	  Evaluation

	  - Beyeler 2017: "Learning to see again: biological constraints on
            cortical plasticity and the implications for sight restoration
            technologies" - retinal but suggests there's a "tolerance envelope" for
            which cortical plasticity can compensate for ( + discussion on
            gamification of training )
**** Post Meeting Notes

   - BibTeX - make sure to check reference consistency!
   - Start from very basics with lit review to ensure the fundamentals are covered
     before talking about more detailed aspects
   - Build argument up from literature review - at the end, should have very clear
     idea of "what needs to be done" in this area
   - BY END OF THIS MONTH - determine /minimum/ work for a good project, and
     /add-ons/ for extra stuff - then do the minimum work first!
   - [ ] TUESDAY DRAFT FOR LITERATURE REVIEW (for discussion next week)
** <2019-04-09 Tue>
*** Meeting Notes :meeting:
**** Status

   Deadlines and timelines:

   #+begin_quote
   [2019-04-17 Wed] - Progress report due - discussed and signed by supervisor.
   [2019-05-01 Wed] - Deadline for oral presentation.
   [2019-05-15 Wed] - Deadline for literature review.
   #+end_quote

   - [ ] Complete progress report
   - Literature review in progress (for discussion)
   - [ ] Ethics documentation
   - Plan for next week:
     - Full draft by Monday next week [2019-04-15 Mon]
     - Practise presentation on Wednesday next week [2019-04-17 Wed]
** <2019-04-17 Wed>

Early overview of slides intention:

[[./presentations/early-overview.png]]

** <2019-04-28 Sun>

Slides for oral presentation done. 

[[./presentations/presentation1.pdf]]

* May 2019

   #+begin_quote
   [2019-05-01 Wed] - Department Oral No. 1.

   [2019-05-15 Wed] - Literature Review due.
   #+end_quote

** <2019-05-01 Wed>

*** Meeting Notes :meeting:

**** Status

Deadlines:

#+begin_quote
[2019-05-15 Wed] - Deadline for literature review.
#+end_quote

**** Plan

1. Literature review draft section/s tonight
2. Set deadline Saturday for literature review to be finished so can start
   working on project
3. Prototype for next week to nut out specific details of methodology of
   psychophysics experiment, including:
   1. Fixation? How long? Randomness?
   2. How long render?
   3. Button press? When?
   4. Feedback? How given?
   5. How long? Locations * trials * time = ???
   6. Do you need to repeat locations?
   7. For the model - do you need feedback? Prevent co-adaptation?
   8. Head fixation and eye tracking
   9. Statistics - maybe non-parametric tests? Offset errors not normally
      distributed exactly (truncated, at the very least)
**** TODOS:

- [ ] Literature review section tonight
- [ ] Literature review full draft end of this week
- [ ] Prototype next week (doesn't need to be complete, just an indication)
** <2019-05-08 Wed>

*** Meeting Notes :meeting:


10 Status

#+begin_quote
[2019-05-15 Wed] - Deadline for literature review.
#+end_quote

- Literature review "draft" (though can't say I'm happy with it yet).
- Formatting + more detail + polishing for next week.
- Prototyping the psychophysics.


**** Timeline Review

***** May

   #+begin_quote
   [2019-05-15 Wed] - Literature Review due.
   #+end_quote

   - Finish and submit literature review.
   - Investigate and iterate network parameters and Psychophysics task, + reassess
   - Recruit participants for experiment.

***** June

   - Finalise experimental design and trial run experiment

***** July

   - Conduct experiment and reiterate if necessary
   - Conduct preliminary analysis of results

***** August

   - Conduct experiment and reiterate if necessary
   - Analyse results
   - Work on thesis

***** September

   #+begin_quote
   [2019-09-25 Wed] - Department Oral No. 2
   #+end_quote

   - Work on thesis
   - Complete draft thesis by end of September, for review

***** October

   #+begin_quote
   [2019-10-29 Tue] - Thesis Due

   [2019-10-29 Tue] - Faculty Oral and Poster upload to Moodle Due

   [2019-10-30 Wed] and [2019-10-31 Thu] - Faculty Oral and Poster Days
   #+end_quote

   - Refine thesis
   - Complete and submit project

**** Post Meeting Notes

   - Lit review - more examples for the last section, rather than just discussion
   - Summary sentences/lead-ins to bind the big lists - more on WHY you are
     discussing things to justify
   - For experiment:
     - [ ] Next week - diagram for the psychophysics part!
     - [ ] How much scramble can the model overcome? Is there a specific type of
       scramble this will work best on?
     - [ ] What is the actual model? What are the actual comparisons? Pick a
       scramble effect.

** <2019-05-15 Wed>

*** Meeting Notes :meeting:

1. Literature review was submitted. I was not happy with it at all. 
2. Post-mortem:
    1. Not enough ties back to hypotheses and aims.
    2. Should have gotten a move on much earlier.
    3. Referencing organisation needs to be better.
3. Focusing on psychophysics aspect this week and getting some sort of experiment running.

** [2019-05-20 Mon]

- Audio files for psychophysics numbers obtained from [Evolution](https://evolution.voxeo.com/library/audio/prompts/numbers/index.jsp) as `.wav` files, licensed under LGPL. Shouldn't be hard to need to change this if required.
- Audio files didn't immediately work with Pygame's sound backend. I converted them to 16-bit signed WAV using Audacity, which fixed the issue.
- With the 10x10 grid of regular phosphenes (not log polar yet) and just brightness based processing, it took me 13 minutes and 9 seconds to finish 30 trials 0f 20 digits each, with feedback.
    - Should I tell people what trial they're up to? Has that been shown in the literature to be bad? It gets a bit frustrating with so many trials.
    - Hard to tell the difference between the 3 and the 9 ( I don't think there even is a difference...)
    - I sometimes mistook the keypress (using the numpad) e.g. mixed up between the 8 and 2 (i.e.  I meant to press 2 but accidentally pressed 8). Is keypad the best input method? I think voice would have less friction, but that would introduce more source of error.
- I finished prototyping the psychophysics part (mostly) and have now got command line arguments up. I just realised I named one of them 'test' (for my test), which will be confusing considering I called the actual experimental phases 'testing' and 'training'...Otherwise, it's working fine.
- TODOS:
    - [ ] Change the randomisation for the digit stream. Block-wise randomisation might mean people can guess at the end of a 10-digit train. 
    - [ ] Different grids. LogPolar, colour grid and interaction grid. 
    - [ ] Processing function - mix and match. 
    - [ ] Process preliminary trial data. 

** [2019-05-21 Tue]

- I spent a bit of time this morning on niceties for the graphical display - fullscreen stimuli with black background and ensuring the aspect ratio was maintained. Also added a window during testing that shows the original image.
- I changed the way that the brightness processing is done, so now it really is like a mask. 
- I'm having a bit of trouble with the polar representation. It really gets completely unrecognisable with 10x10 electrodes, and even at 30x30 electrodes (where it lags a lot), it's not really recognisable at all. I think the "mask" approach isn't very good for a polar grid...
- Okay, I retranslated it so that the center is actually in the center, and it's not so bad now. The scaling for a log polar grid is a bit of an issue though. 

** [2019-05-22 Wed]

*** Initial Analysis

**** Imports
     :PROPERTIES:
     :CUSTOM_ID: imports
     :END:

   #+BEGIN_SRC python
       import numpy as np
       import pandas as pd
       import json
       import os
       from matplotlib import pyplot as plt
       from glob import glob

       %matplotlib inline
   #+END_SRC

**** Data Reading
     :PROPERTIES:
     :CUSTOM_ID: data-reading
     :END:

   Specifying the directory containing the participant data.

   #+BEGIN_SRC python
       DATA_DIR = '../data/sessions/participants/'
   #+END_SRC

   Specifying the fileformat glob of the config files.

   #+BEGIN_SRC python
       datafiles = glob(DATA_DIR+'*.json')
   #+END_SRC

   Looping through each config file and reading the data into a variable.

   #+BEGIN_SRC python
       configs = []
       for file in datafiles:
           with open(file) as infile:
               configs.append(json.load(infile))
   #+END_SRC

   #+BEGIN_SRC python
       list(configs[0].keys())
   #+END_SRC

   #+BEGIN_EXAMPLE
       ['TESTING',
	'NTRIALS',
	'NCUES',
	'GRID_TYPE',
	'NO_NUMPAD',
	'XSIZE',
	'YSIZE',
	'SCALE',
	'EXSIZE',
	'EYSIZE',
	'IMAGE_TEMPLATE',
	'IMAGE_SIZE',
	'IMAGE_SCALE',
	'DATETIME_FORMAT',
	'DIGIT_SOUND_TEMPLATE',
	'CONFIG_FILE_TEMPLATE',
	'SESSION_FILE_TEMPLATE',
	'CORRECT_NOTE',
	'INCORRECT_NOTE',
	'NOTE_DURATION',
	'NOTE_VOLUME',
	'SESSION_VARS',
	'SESSION_HEADER',
	'SESSION_ROW_TEMPLATE',
	'PROMPT_TEXT',
	'END_TEXT',
	'KEY_LIST',
	'EXCLUDED',
	'details',
	'configFile',
	'sessionFile']
   #+END_EXAMPLE

   Each config in =configs= contains the name of the session file, relative
   to root directory of =03-psychophysics=. First, we change the current
   directory.

   #+BEGIN_SRC python
       os.chdir('../')
   #+END_SRC

   Then read all the file data.

   #+BEGIN_SRC python
       data = [pd.read_csv(config['sessionFile']) for config in configs]
   #+END_SRC

   And examining the first ten rows of the first data file:

   #+BEGIN_SRC python
       data[0][:10]
   #+END_SRC

   #+BEGIN_HTML
     <div>
   #+END_HTML

   #+BEGIN_HTML
     <style scoped>
	 .dataframe tbody tr th:only-of-type {
             vertical-align: middle;
	 }

	 .dataframe tbody tr th {
             vertical-align: top;
	 }

	 .dataframe thead th {
             text-align: right;
	 }
     </style>
   #+END_HTML

   #+BEGIN_HTML
     <table border="1" class="dataframe">
   #+END_HTML

   #+BEGIN_HTML
     <thead>
   #+END_HTML

   #+BEGIN_HTML
     <tr style="text-align: right;">
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   trial

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   cue

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   digit

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   keypress

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   cuetime

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   trialtime

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   sessiontime

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     </thead>
   #+END_HTML

   #+BEGIN_HTML
     <tbody>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   0

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1.827028

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   2.579587

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   3.309634

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   1

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   7

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   7

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1.226297

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   3.805941

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   4.535988

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   2

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   2

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   9

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   9

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   2.117782

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   5.923782

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   6.653828

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   3

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   3

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   3

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1.098034

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   7.021869

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   7.751917

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   4

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   4

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   8

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   8

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1.255164

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   8.277109

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   9.007155

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   5

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   5

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   2

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   2

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0.846393

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   9.123560

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   9.853607

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   6

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   6

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   4

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   4

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1.088721

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   10.212334

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   10.942381

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   7

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   7

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   5

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   5

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1.343659

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   11.556046

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   12.286093

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   8

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   8

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1.031067

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   12.587172

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   13.317219

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     <tr>
   #+END_HTML

   #+BEGIN_HTML
     <th>
   #+END_HTML

   9

   #+BEGIN_HTML
     </th>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   0

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   9

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   6

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   6

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   1.568590

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   14.155817

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     <td>
   #+END_HTML

   14.885864

   #+BEGIN_HTML
     </td>
   #+END_HTML

   #+BEGIN_HTML
     </tr>
   #+END_HTML

   #+BEGIN_HTML
     </tbody>
   #+END_HTML

   #+BEGIN_HTML
     </table>
   #+END_HTML

   #+BEGIN_HTML
     </div>
   #+END_HTML

   For convenience, I will zip the configs and data.

   #+BEGIN_SRC python
       alldata = list(zip(configs, data))
   #+END_SRC

   At the moment, I only have data to compare GRID (i.e.renderers), as
   I've only implemented one processor.

   #+BEGIN_SRC python
       ngrids = len(data)
   #+END_SRC

**** What was the mean accuracy of digit recognition?
     :PROPERTIES:
     :CUSTOM_ID: what-was-the-mean-accuracy-of-digit-recognition
     :END:

   First, we find the mean accuracy of each method, overall.

   #+BEGIN_SRC python
       meanAccuracies = [sum(d.digit == d.keypress) / len(d) for d in data]
   #+END_SRC

   #+BEGIN_SRC python
       gridTypes = [c['GRID_TYPE'] for c in configs]
   #+END_SRC

   #+BEGIN_SRC python
       plt.bar(range(ngrids), meanAccuracies, tick_label=gridTypes)
       plt.axhline(1 / 10, linestyle=':', color='r')
       plt.title("Mean Accuracy vs Grid Render Method")
   #+END_SRC

   Text(0.5, 1.0, 'Mean Accuracy vs Grid Render Method')


   [[./03-psychophysics/data/archive/mean-accuracy-vs-grid-render.png]]

**** What was the mean response time to a cue?
     :PROPERTIES:
     :CUSTOM_ID: what-was-the-mean-response-time-to-a-cue
     :END:

   #+BEGIN_SRC python
       meanResponseTime = [d.cuetime.mean() for d in data]
       stdResponseTime = [d.cuetime.std() for d in data]
   #+END_SRC

   #+BEGIN_SRC python
       plt.bar(range(ngrids), meanResponseTime, tick_label=gridTypes, yerr=stdResponseTime)
       plt.title("Mean Response Time (ssec) vs Grid Render Method.")
   #+END_SRC

   Text(0.5, 1.0, 'Mean Response Time (ssec) vs Grid Render Method.')


   [[./03-psychophysics/data/archive/mean-response-time.png]]

**** How did performance change over a session?
     :PROPERTIES:
     :CUSTOM_ID: how-did-performance-change-over-a-session
     :END:

   #+BEGIN_SRC python
       fig, ax = plt.subplots(4, sharex=True, sharey=True, figsize=(5,10))

       for i, d in enumerate(data):
           ax[i].step(range(len(d)), np.cumsum((d['digit'] == d['keypress']).astype(int)))
           ax[i].set_title(gridTypes[i])
        
       fig.suptitle("Cumulative performance during a Session")
   #+END_SRC

   Text(0.5, 0.98, 'Cumulative performance during a Session')


   [[./03-psychophysics/data/archive/cumulative-performance.png]]

**** What numbers were chosen the most?
     :PROPERTIES:
     :CUSTOM_ID: what-numbers-were-chosen-the-most
     :END:

   #+BEGIN_SRC python
       fig, ax = plt.subplots(4, sharex=True, sharey=True, figsize=(5,8))

       for i, d in enumerate(data):
           ax[i].bar(range(10), [sum((d.keypress == digit).astype(int)) for digit in range(10)], tick_label=range(10))
           ax[i].axhline(10, color='r', linestyle=':')
           ax[i].set_title(gridTypes[i])
   #+END_SRC

   [[./03-psychophysics/data/archive/number-frequency-chosen.png]]

**** What numbers were most well recognised?
     :PROPERTIES:
     :CUSTOM_ID: what-numbers-were-most-well-recognised
     :END:

   #+BEGIN_SRC python
       for d in data:
           d['correct'] = (d.digit == d.keypress).astype(int)
   #+END_SRC

   #+BEGIN_SRC python
       ncues = 10
   #+END_SRC

   #+BEGIN_SRC python
       proportions = [[sum(((d.digit == i) & (d.correct == 1)).astype(int)) / ncues for i in range(10)] for d in data]
   #+END_SRC

   #+BEGIN_SRC python
       fig, ax = plt.subplots(ngrids, sharex=True, sharey=True, figsize=(5, 8))

       for i, props in enumerate(proportions):
           ax[i].bar(range(10), props, tick_label=range(10))
           ax[i].set_title(gridTypes[i])

       fig.suptitle('Accuracy per digit for each Grid')
   #+END_SRC

   Text(0.5, 0.98, 'Accuracy per digit for each Grid')


   [[./03-psychophysics/data/archive/number-well-recognised.png]]

**** What numbers were often confused?
     :PROPERTIES:
     :CUSTOM_ID: what-numbers-were-often-confused
     :END:

   #+BEGIN_SRC python
       from collections import Counter
   #+END_SRC

   #+BEGIN_SRC python
       fig, ax = plt.subplots(4, figsize=(8,8))

       for i, d in enumerate(data):
           keypairs = []
           for row in d.iterrows():
	       keypair = tuple(frozenset((int(row[1].digit), int(row[1].keypress))))
	       keypairs.append(keypair)
           c = Counter(keypairs).most_common()
           ax[i].bar(range(len(c)), [el[1] for el in c], tick_label=[str(el[0]) for el in c])
           ax[i].set_title(gridTypes[i])
           for tick in ax[i].get_xticklabels():
	       tick.set_rotation(90)
       fig.tight_layout()
   #+END_SRC


   [[./03-psychophysics/data/archive/number-confused.png]]

*** Meeting Notes :meeting:

1. Concrete examples of what the CNN can do.
2. Changing digit position/size?
3. One hemisphere of phosphenes only.
4. Keep feedback - good for psychophysics.
5. Can be faster!
6. Can throw away first however many trials based on sliding window of accuracies - or weighting the inputs to the network based on performance.
7. Confusion matrix to present the pairs of digits which were often confused.
8. Could GANs be applied?
9. To make a distinguishing network - maybe not only optimise for the difference and distinguishability, but cull candidate processors based on similarity to the original images. 
10. Maybe use larger digits then downsample - aliasing information may be helpful information.

** [2019-05-27 Mon]

- Worked on getting larger digits. Briefly looked at changing the position of digits, but I think classification gets very difficult with that approach. But it's easy enough and I've written it so I might come back to it.
- Machine decoder (completely naive) can get up to 40% accuracy with moving digits on a polar regular grid. I guess that's not so bad.

** [2019-05-28 Tue]

- When the digit isn't moving, a CNN can achieve 100% accuracy, though I guess tthat's not completely unexpected.
- I just tried again (with some more constraint to position i.e. within 0.25 and 0.75 of the x and y axis) and the model can get around 50% accuracy (still naive).
- Tensorflow versioning is a little bit frustrating. I am upgrading to tensorflow 2.0 alpha. 

** [2019-05-29 Wed]

- I decided to implement mouse tracking to better simulate "real life"
  situations where you have the option to move your head. It's actually not
  that hard to get decent performance even with the irregular grid though...And
  I can bet that the CNNs aren't going to preserve spatial information very
  well.
- I'm not positive the gradient descent will actually work at all with a human.
  That, I still need to think about... But it seems like it should be possible?
  After all, GAN's are possible and I'm not sure I believe the gradient is
  tracked all the way through the decoder network. Maybe it is?
- I guess, after trying the experiment with mouse tracking and a non-linear
  grid, the question really is about interpretability. I think my very first
  idea a few months ago, that user-specific context is important, is probably
  the better idea actually... There are going to be intrinsic information 
  limitations and something as simple as positional sense may be easier.

*** Progress
- Working on CNN
  - Decoder trained on regular grid 100% (as expected), regularunique grid ~ 90%
  - Encoder network with random inital weights can be fed inputs and get
    correctly shaped outputs
  - Currently working on the training step, going through Tensorflow
    documentation (gradient tracking is a bit opaque)
- Numerous improvements to experiment
  - Using aliased digits.
  - Can vary digit position per trial, though I have chosen not to for now.
  - Half-visual field renders.
  - Less latency on render (lower resolution to assist the computer decoder),
    will facilitate more data
  - Area based 
- Plan for this Week
  - Finish encoder training model
  - Investigate loss functions (binary vs rated digits?)
   
*** Meeting Notes
  
  - Short meeting
  - Size and rotation of digits 
  - Confidence - opt out - opt out - 0, -1, or 1

** [2019-05-30 Thu]
*** May 30th: Naive GAN                                        :generative:
     :PROPERTIES:
     :CUSTOM_ID: may-30th-naive-gan
     :END:

  | Property          | Value                  |
  |-------------------+------------------------|
  | Git Commit Hash   | None (run on colab)    |
  | Grid              | Polar Regular Unique   |
  | Vector size       | 12 $\times$ 12 = 144   |

  #+BEGIN_SRC ditaa
                    +--------------------------------------------+--------------------------+
                    |                                            |                          |
                    v                                            v                          |
                  Encoder               Renderer              Decoder                       |
      100-vector ---------> 144-vector ----------> 2D Image  --------> Class (binary) -+    |
											- Loss
                                                   MNIST Im. --------> Class (binary) -+
                      
  #+END_SRC

  First attempt was just a completely naive GAN that attempted to
  transform a random seed (100-vector of random floats) into a 144-vector
  of floats that, when fed to the renderer, would best look like a digit.
  As this was dissociated from the actual digit identity, this was purely
  a feasibility test.

  It looked like the below after 5 epochs (total 202 seconds, after which
  I stopped it).

  [[./03-psychophysics/data/archive/naive-gan.png]]

  Lessons learnt:

  - Yes, training on the renders /can/ be done (though it required a
    number of modifications which became clear during this feasibility
    test, including the need to pre-render all the electrodes and wrap the
    final render under Tensorflow).
  - In this particular case, you can see /mode collapse/ (i.e.they all
    look the same! even though the random seeds are different). Probably
    won't be such a big issue when takes the actual digit, rather than a
    random seed, as the input.
  - Training takes quite a long time.
  - Does not seem likely that training based on participant performance
    will be feasible, given the huge data and time requirements...but it
    definitely seems plausible to train based on individual electrode
    features and certain electrode combining functions, which could be
    modelled after individuals' perceptions.

*** Timeline

- JUN 7: Finish the encoder. 
- JUN 14: Test experiment on self
- JUN 21: Analyse self results.
- JUN 28: Finish experiment setup for psychophysics testing.
- JUL 5: Recruit 5 participants and conduct experiments.
- JUL 19: Finish conducting experiments.
- AUG 2: Analyse results for 1) Significant difference between processors, 2) Significant difference between renderers.

* June 2019
** [2019-06-03 Mon]

- I worked on a GAN (full GAN i.e. both the encoder and decoder are being trained), which very closely follows a basic GAN architecture. It can produce actually not-too-shabby looking pictograms from random noise, but I haven't yet linked these to the actual digits (i.e. it produces random digits out of random noises, but at least they sort of look like digits). I think the next step is going to be actually making it linkable with the digit themselves...
- Ah! I've actually managed to get the "half-encoder" model to run. I don't know the results yet, but at the very least ,it's running without error. It basically trains an encoder which takes 64x64 phosphenised images then runs it through a pretrained MNIST decoder, comparing it to the original 64x64 image run through the pretrained MNIST decoder. Loss is CategoricalCrossEntropy at the moment (I can't seem to get tf.argmax to work even though I think it would be better because it would discard all the incorrect digit confidences; using argmax causes the trainer to lose track of the gradients so I get the dreaded "no gradients for any variable" error) but at the very least it is running. It is taking a long time though....
- I'm doing 12 batches of 128 digits per epoch at the moment for 1536 per epoch, and it takes about one minute per epoch (so 20 minutes for 20 epochs). I can't say that the result look terribly good though... (but at least there are results)
- After 19 such epochs, the results are...interesting. But they don't look very digit like...I wonder whether I should just separate out the digit interpretation and the encoding step. But I have to think about that.
- It turns out that argmax isn't differentiable, which explains why the gradients were lost. But using one hot might work....maybe?
- Backup plan might be good. Probably the 'selective sight' plan.
*** June 3rd: Generative Network against MNIST                 :generative:

So this is part of a series of attempts at digit generation.

  The original digit images look like this:

  [[./03-psychophysics/data/archive/original-digit.png]]

  The aim is to develop a network which can process this image and output
  a *vector* (/not/ an image), which when subsequently fed to a specified
  distorted renderer, will produce an image that a human can infer the
  original digit from. The renderer does not change, so we can (hopefully)
  train a network to best work with the renderer it gets.

     :PROPERTIES:
     :CUSTOM_ID: june-3rd-generative-network-against-mnist
     :END:

  | Property          | Value                  |
  |-------------------+------------------------|
  | Git Commit Hash   | None (run on colab)    |
  | Grid              | Polar Regular          |
  | Vector size       | 12 $\times$ 12 = 144   |

  #+BEGIN_SRC ditaa

                   +-------------------------------------------------------------+
                   |                                                             |
                   v                                                             |
		Encoder               Renderer           MNIST Decoder           |
      2D Image ---------> 144-vector ----------> 2D Image ---------> Class --+   |
                                                                              - Loss
      Image Label -----------------------------------------------------------+

  #+END_SRC

  Second attempt was a generative network which directly took the digit
  image and spit out the 144-vector, which was then fed to the renderer. I
  trained a separate decoder neural network on the MNIST digit dataset,
  and fed the rendered image to this stable pretrained network, and
  optimised against the predictions of the pretrained network on the
  renderer.

  Network architectures at this point were very ad-hoc; this was still in
  the realm of "feasibility testing."

  It looks like the below after 11 epochs (approx 80 seconds):

  [[./03-psychophysics/data/archive/generative-network-mnist.png]]

  Lessons learnt:

  - Yes, training on digit identities is feasible.
  - Doesn't really look like anything except garbage.
  - Training is slow.

** [2019-06-05 Wed]

- Preliminary testing with the new attempt (attempt 3) seems to train
  very slowly. Trying again with a one-hot comparison for the loss
  function, which I think captures the meaning a bit more nicely...but
  the change between epochs isn't really very much for some reason.
- Changing the learning rate speeds up the learning speed, although it
  seems a bit slow still. Using the AdamOptimiser, maybe changing
  epsilon would work?
- One of the other mysteries is what architecture is best. I've only
  got two layers after the softmax layer at the moment (and one of
  those layers is the output layer!).
- Also, maybe I should be training on more data. At the moment, I'm
  only using 3072 per epoch, but I know the MNIST set uses 60000 per
  epoch. Time is just the issue.
- I've moved out the digit generation of the training loop because I
  think it adds a lot of time (generating one digit image takes 12ms,
  so 10000 takes 120 seconds or 2 minutes. That's 2 minutes
  (hopefully) saved?). So I've pre-generated these images and saved
  them and am just feeding them into the training loop. One epoch with
  10000 images takes 118 seconds (as opposed to 6000 images which took
  roughly the same time). I think I might have to make it a Tensorflow
  Dataset at some point though, I'm literally just feeding it Numpy
  array slices...
- I need to actually check that the "render tensor" method of the grid
  is equivalent to the "render" method, otherwise my visualisations
  may not actually reflect the performance of the generation...(Okay,
  just checked and can confirm that render and render tensor both give
  the same output. Phew!)
- So I realised that increasing epsilon actually decreases the
  change...and when I had a learning rate of 1 and the default
  epsilon, woah! The network really blasted the brightness levels so I
  got almost a completely black and white image. Didn't see that
  before.
- I realise the generator might also be learning things to do with the
  inadequacies of the MNIST decoder, which is pretty naive...hm. I
  think whatever I compare against as the decoder needs to actually be
  good and "human-like" (or "visual-like").
- This seems more likely given that the accuracy (which is not what is
  optimised against, since it's optimising against cross entropy which
  hopefully should capture near-equivocal classifications a bit
  better) improves dramatically after the first epoch. So it "works"
  for the MNIST decoder - it's just the MNIST decoder isn't very good
  at distinguishing garbage. So the "representation" seems to learn
  features...Maybe this needs to be hooked in with another
  discriminator network.  (I mean, it gets an "accuracy" of 0.985
  after the first epoch from 0.091 before training, when tested
  against the first 1000 digit images. Hm.)
- Moral of the story - it's training really well!...just the thing
  it's training against isn't a very knowledgeable decoder itself.
- I've set it up to now output the losses instead of accuracy. Loss
  decreases quite nicely. I think the real challenge now is figuring
  out how to make the decoder..."human-like".
- I've added an extra class, a garbage 11th class, to the MNIST
  decoder. So now hopefully garbage renders get classiifed as garbage
  instead of digits and adds some extra adversity to the generative
  network...
*** June 5th: Decoder -> Generative Network against MNIST      :generative:
     :PROPERTIES:
     :CUSTOM_ID: june-5th-decoder---generative-network-against-mnist
     :END:

  | Property          | Value                                      |
  |-------------------+--------------------------------------------|
  | Git Commit Hash   | 71bd2ef6e66122935e320f0880658e7442bc832c   |
  | Grid              | Polar Regular                              |
  | Vector size       | 12 $\times$ 12 = 144                       |

  Third attempt was a generative neural network which first decodes the
  digit using a pretrained decoder, then upsamples that decoded digit into
  a 144 vector, which is fed into a renderer and then decoded by an MNIST
  decoder.

  #+BEGIN_SRC ditaa

                                     +-------------------------------------------------------------+
                                     |                                                             |
                                     v                                                             |
		Digit Decoder ---> Encoder              Renderer           MNIST Decoder           |
      2D Image ---------------------------> 144-vector ----------> 2D Image ---------> Class --+   |
												- Loss
      Image Label -----------------------------------------------------------------------------+

  #+END_SRC

  Before training:

  [[./03-psychophysics/data/archive/decoder-generative-mnist-pre.png]]

  After training for 10 epochs, approx 5 minutes:

  [[./03-psychophysics/data/archive/decoder-generative-mnist-post.png]]

  Lessons learnt:

  - Not shown here, but loss actually minimised really well i.e.the MNIST
    decoder was able to decode the digit from these representations with
    high accuracy (> 90%!). So, while the encoder is learning well, it is
    learning in a way optimised for the MNIST decoder features. Some of
    the features may be useful, but mostly not really.
  - As the images are decoded first, essentially each digit just maps to a
    single phosphene representation. Arguable whether a CNN is needed to
    achieve this (but still interesting...)
  - Had a go messing with learning rates and oher hyperparameters here.
    May make a difference.

*** June 5th: Decoder -> Generative Network against MNIST + Garbage :generative:
     :PROPERTIES:
     :CUSTOM_ID: june-5th-decoder---generative-network-against-mnist-garbage
     :END:

  | Property          | Value                                      |
  |-------------------+--------------------------------------------|
  | Git Commit Hash   | e7f41258e408a15d1138b1a80bb97bace204363e   |
  | Grid              | Polar Regular                              |
  | Vector size       | 12 $\times$ 12 = 144                       |

  Because the output of the third attempt was learning features but
  ultimately still outputting garbageful renderers, I decided to add an
  additional class to the MNIST decoder - a "garbage" class full of random
  renders of 144-vectors. The architecture is otherwise very similar, just
  the decoding step at the end uses a decoder which can output 11
  classes - and none of the "real" labels are of the 11th class, so
  hopefully the encoder should optimise /away/ from garbage renders.

  #+BEGIN_SRC ditaa

                                     +-------------------------------------------------------------+
                                     |                                                             |
                                     v                                                             |
		Digit Decoder ---> Encoder              Renderer     MNIST + Garbage Decoder       |
      2D Image ---------------------------> 144-vector ----------> 2D Image ---------> Class --+   |
												- Loss
      Image Label -----------------------------------------------------------------------------+

  #+END_SRC

  Before training:

  [[./03-psychophysics/data/archive/decoder-generative-mnist-garbage-pre.png]]

  After training on 50 epochs (approx 40 minutes, though the training
  slows dramatically after the first 10 or so epochs):

  [[./03-psychophysics/data/archive/decoder-generative-mnist-garbage-post.png]]

  And losses:
  [[./03-psychophysics/data/archive/decoder-generative-mnist-garbage-loss.png]]

  Lessons learnt:

  - First time actually the output renders actually are somewhat
    encouraging (although they are far from human-interpretable, except
    for the 1!). Slightly more useful features are being learned, although
    clearly the network still optimises against the decoder nuances a
    little too much. But definitely better putting the "A" back in GAN!
  - How to choose hyperparameters? Big question.
  - For this particular run, I chose a learning rate of 1e-2. Larger
    learning rates cause more drastic changes on the first epoch.

*** Progress


- Started attempts at training on simulated renders. Successfully trainable, results vary (though it took me a bit of tinkering...)
    - Attempt 1: purely generative from random seed ("what can the network actually produce?"). Produced sort of digit-like forms but experienced some mode collapse (i.e. they all looked the same) - not entirely unexpected, very naive architecture.
    - Attempt 2: trained a neural network processor from scratch and fed to a stable MNIST decoder, loss defined as categorical cross-entropy against MNIST predictions. Produces sort of digit-like forms (again), but does not appear to scale very well with digit position. Probably because the network is trying to do too much at once. Also training stagnates very quickly.
    - Attempt 3: pretrained digit classifier, then fed to a generative neural network (which is the only trainable network), then fed to a stable MNIST decoder. Results pending.
- Timeline:
    - Jun 21st - Experimental setup progress - should be almost done.
    - Jun 28th - MUST FINISH EXPERIMENTAL SETUP.
- Looking forward:
    - I'm a bit doubtful that training on participant data will be feasible (unlikely to be able to get enough data to be useful, and issue of choice of backpropogation/loss) - probably best to keep the training "offline" (i.e. feed the neural network a map and renderer) and test the offline version. So the experimental setup will be comparing two different, stable processors on digit recognition accuracy and speed + control. 
        - At the moment, experimental setup looks to be:
            1. Recruit participant
            2. Generate a unique map (spatial) & ?renderer (nonLinear?) for participant
            3. Train generative network on simulated renderer with map
            4. Test map with a brightness processor vs control processor  vs generative network processor
            5. Compare accuracies, speed, and scanning motions between processors
    - In coming- week, I will work on investigating further architectures for the generative model. (this week was getting things actually working, coming week will be about seeing what works best.)
    - Backup plan in back of my mind...(based on object localisation only like the key detection demo). This would involve hooking up pretrained imagenext networks into an experiment, which involves much less unknowns (literally just putting the captured bounding boxes under the renderer), but less interesting (but potentially more "realistically useful?"). 

- For discussion:
    - Thinking about actual utility - is identity information even useful if location is not preserved? (contrasting with, say, location, which is difficult to communicate concisely through other modalities). 
    - Comparison groups - good enough? (e.g. should I be testing a processor which is a CNN with hand-encoded representations?) 
    - Is hooking up with the VR headset easier/better than using mouse movements to simulate scanning? (could also open up possibility for more sophisticated tasks which leverage pretrained networks and would also be better for the object detection backup plan...)
    - Quick question - how important is it that I catalogue/retain the code for my failed attempts? It's all version controlled with Git, but should I be doing this more explicitly?

** [2019-06-11 Tue]

- Been really busy this week with a number of other responsibilities,
  so not much has changed yet...
- I'm just going to quickly try adding another conv1d layer to the
  MNIST digit network. Adding layers doesn't increase performance of
  MNIST categorisation all that much (it's getting around 97%/98%),
  but I'm not sure if the filters might be more useful.
- I feel like I should look into a method to visualise the filters -
  that might be useful.
- The learning rate does seem to make a large difference - if it's too
  large, it really bleaches the image on the first epoch, and there
  aren't that many changes after the first epoch is done (although the
  loss doesn't decrease to as low a level, so maybe there's still
  hope?)
- I think I should so some extra reading on techniques used for
  GANs. That might give me some better ideas on how exactly these
  hyperparamters should be tuned.
- I'm also wondering on how the architecture of the encoder should
  look like. I guess I originally thought that since it could just be
  a one-to-one mapping, that a simple architecture (even without
  hidden layers) might work. But I wonder whether that limits the
  jumps that training can make?
- I'm trying to reorganise my directory a little bit. I think it's
  better to actually save everything between runs and keep them
  persisted, rather than saving them through git commits and
  overwriting them. That way, I can easily access past results without
  having to rollback to a commit.
- I made the index notebook more general so I can specify different
  grid parameters without changing the code too much. I also made a
  gif of the training at each epoch. That should make it easier to
  visualise what's going on.
- I'm wondering whether it might actually be beneficial for me to just
  stitch together all my logbook entries into one lab notebook
  document. Actually, I think that would be good. Probably an org mode
  file.

*** June 11th: Upping the Decoder Layers and Learning Rate :generative:
     :PROPERTIES:
     :CUSTOM_ID: june-11th-upping-the-decoder-layers-and-learning-rate
     :END:

  | Property          | Value                                      |
  |-------------------+--------------------------------------------|
  | Git Commit Hash   | 7d24bc3f3f2e2ef883d8f688799536b4ae3ea9fc   |
  | Grid              | Polar Regular                              |
  | Vector size       | 12 $\times$ 12 = 144                       |

  I used the same architecture as above, but I wanted to see what would
  happen if I:

  1. Upped the number of layers in the MNIST network (I added one more
     Conv1d layer)
  2. Upping the learning rate (to 1e-1).

  Training gif:

  #+CAPTION: GIF
 [[./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-11_10-47_evolution_PolarRegularGrid_12-12_64-64.gif]]

  Plotted loss:

  [[./03-psychophysics/data/archive/decoder-generative-mnist-garbage-uplearn-loss.png]]

  Lessons learnt:

  1. The learning rate seems very aggressive (the loss shoots up
     initially, and indeed after the first few epochs, some of the digits
     are entirely blank).
  2. Seems to really "bleach" the digits.
  3. Loss decreases but not to the level of a slower learning rate.
  4. I get the impression that if I were to continue this beyond 50
     epochs, I might get better results.

*** Directory Reorganisation 

- [2019-06-11 Tue 13:53] :: I've done a complete reorganisation of the
     directories so now my lab notebook is all in one place. I think
     I'm going to try logging things much better from now on, it's
     felt a bit disorganised. I've also noticed that it's much easier
     to see the gaps when it's all laid out chronologically...

*** Plan Review

I think I'd like to try by tomorrow:

- [ ] Visualising the filters of the MNIST decoder +/- trained encoder
- [ ] Integrating the trained encoder with the digit experiment
- [ ] Running the encoder training for the non-linear grid (this is
  going to be the most important part, and possibly the most difficult
  part too.)
- [ ] Testing my own accuracy for different processors
- [ ] Analysing the data for mouse position, accuracy etc.

*** Adding a Layer to the Generative Encoder Network
:LOGBOOK:
CLOCK: [2019-06-11 Tue 14:09]--[2019-06-11 Tue 14:47] =>  0:38
:END:

| Git Commit Hash | 3da6687fb8cdf4f0e7d4fedfebcb1689fea6dc57 |
| Grid            | Polar Regular                            |
| Vector size     | 12 x 12 = 144                            |


I figure I might as well see what happens if I add a another layer to
the /encoder/ network, so now its architecture is the following:

#+BEGIN_SRC 
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
sequential_16 (Sequential)   (None, 10)                164634    
_________________________________________________________________
dense_7 (Dense)              (None, 12)                132       
_________________________________________________________________
dense_8 (Dense)              (None, 12)                156       
_________________________________________________________________
dense_9 (Dense)              (None, 144)               1872      
=================================================================
Total params: 166,794
Trainable params: 2,160
Non-trainable params: 164,634
_________________________________________________________________
#+END_SRC

I've also reset the learning rate to 1e-2 rather than 1e-1, as it
seems like 1e-1 moves a little too fast.

After just 3 epochs, the loss is already down to 0.05ish
numbers. Hm. I think the main issue is the question of how exactly do
you get the model which this is training adversarially against to best
replicate a human? The issue right now is that, while the digits may
be separable, they don't "look" digit like, so I suppose there needs
to be some sort of quantification of what "looks" digit like that is
then incorporated into the loss function. That's what the MNIST
decoder is supposed to do, but I suppose it's really more a feature
extractor than a "this is what the whole image needs to look like"
type of thing. 

Maybe if I increase the garbage dataset, it might start looking a bit
better. The results are actually not terrible, but they could
definitely look better (but I suppose I'll need to run this for the
non-linear, irregular grid before it starts being meaningful).

Each epoch is taking approximately 40 seconds, which is fine for the
moment (so 50 epochs = 2000 seconds, ~30 minutes). 

Actually, in hindsight, perhaps reducing the parameters is the better
idea - even just making it a two layer network. I think more
parameters has the danger of getting stuck...

After 25 epochs, the loss is 0.04ish. I just decided to stop it there
because I didn't think it was necessarily going to give any better
results.

Here's the gif:

[[./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-11_14-13_evolution_PolarRegularGrid_12-12_64-64.gif]]

And the loss graph:

[[file:./03-psychophysics/./data/training-intermediate-data/training-graphs/2019-06-11_14-13_loss_PolarRegularGrid_12-12_64-64.png]]

I guess it really shows that while optimisation is occuring fairly
alright, the decoder it trains against needs to be fairly good at
distinguishing garbage from non-garbage. 

*** Training the MNIST Decoder on More Garbage + Larger Kernels
:LOGBOOK:
CLOCK: [2019-06-11 Tue 19:39]--[2019-06-11 Tue 22:22] =>  2:43
CLOCK: [2019-06-11 Tue 14:50]--[2019-06-11 Tue 19:39] =>  4:49
:END:

| Git Commit Hash |  171fb8ce79b00e660ff43188ba04d8498a9d3c34 |
| Grid | Polar Regular | 
| Vector size | 12 x 12 = 144 | 

So since it all depends on the decoder, I might as well if I can train
the decoder to recognise the garbage better.

I previously trained the decoder on =n//10= garbage samples, where n
is the number of MNIST digits. I'll try training the decoder on =n=
garbage samples (i.e. there are 10 times as many garbage samples as
there are for any one individual digit), and see how that goes. I
think this will take quite a while.

I will also decrease the encoder back down to the decoder model + one
output vector layer of 144. 

---

So training the MNIST decoder again - I think the "accuracy" may be a
bit deceptive now that half the set of 120,000 digits is
garbage. Hopefully, at the very least, this network will get very good
at detecting garbage. 

---

I've visualised the kernels and they're actually somewhat edge like in
the first layer. After that, since they're taking the input from
kernels rather than the image, they no longer look as nice since of
course they are now working on a 3D volume of kernels rather than the
image, but I think it's still interesting to visualise. 

--- 

I've actually just realised I could have been using 2D convolutional
kernels for the layers instead of 1! All I had to do was reshape the
input to add an extra dimension. I don't know if this will give better
results, but I feel like it might - since it's now working on the 2D
representation of the image...I hope. Training the MNIST decoder takes
quite a while now, but I think it may be worth it. Hopefully. 

Yan's recommended me take every 2 or so days to self-reflect and write
my ideas down and re-evaluate. I think I'll do that, I've been really
meaning to try and give some order to what I'm going, I feel like
things are a little bit messy at the moment. 

--- 

I left it to train and looking at the filters learned are quite
interesting. It's not the novel part, but still interesting.

These are the filters from the first 2D convolutional layer:

[[./03-psychophysics/data/archive/kernel-shapes-mnist-2d.png]]

This is important because this is what the decoder is training against
(at least, in my opinion).

Okay, I'm running another run with this 2D convolutional MNIST network
that's been trained on 10 times more garbage samples. Fingers crossed!

Log file for this run is
[[./03-psychophysics\data\training-intermediate-data\training-losses\2019-06-11_19-00_loss_PolarRegularGrid_12-12_64-64.log][2019-06-11_19-00_loss_PolarRegularGrid_12-12_64-64.log]].

After 2 epochs, it seems to be quite a bit better already! Hopefully
it stays this way...Learning rate is 1e-2 so hopefully doesn't
collapse. Training is about 50 seconds per epoch, acceptable still.

Okay, it doesn't really change all that much after the first 8 or so
epochs. Well that's expected I guess.

Here's the gif:

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-11_19-00_evolution_PolarRegularGrid_12-12_64-64.gif]]

And the loss plot:

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-11_19-00_loss_PolarRegularGrid_12-12_64-64.png]]

The last loss is 0.033ish.

Much better tha nthe output I had previously, though stil could be
better. But I suppose I should account for the fact that the decoder
at the start of the network is not always correct either.

I saved the grid as
=2019-06-11_19-00_grid_PolarRegularGrid_12-12_64-64.pkl= and the
encoder as
=2019-06-11_19-00_encoder_PolarRegularGrid_12-12_64-64.h5=. 

*** Trying the Non-Linear Grid + Upping the Learning Rate Slightly

I think it's time to see how this works for something for which it
might actually be useful - and which it might actually fair better at
than humans (possibly). 

I'll:
- Make the learning rate 5e-2, slightly more aggressive
- Use the nonLinear grid
- Same no-medium input architecture as before (i.e. no trainable
  hidden layers). I actually wonder whether maybe hidden layers would
  help the network be resilient against the imperfect iniital
  decoder...not sure yet. Maybe I should try to improve the initial
  decoder.

It seems to be grey around the whole image. Not entirely sure why...
Ah. I just realised, I need to retrian the MNIST decoder since that's
where the garbage comes from!

Okay, while I train the MNIST decoder, I might as well train the digit
recogniser again.

Ahh, I'm getting what looks like much much better accuracy this time
for the digit decoder. That's good. Maybe that'll help things? I upped
the training data to 10000 and added a convolutional layer and
increased the kernel size, and it gets 100% accuracy, so much better
than the 97% ish before.

I started training and was wondering why the loss was decreasing
well but the images looked pretty washed out with grey. I realised it
was just because of the plotting function I was using for
visualisation, I needed to set vmin=-1 and vmax=1 to ensure that the
plot didn't autoscale to the lowest and highest values (which was a
bigger problem for this grid because phosphenes combine through
rescaling...). I have to think about whether this has any large
implications. 

Actually, now I'm a little bit puzzled. It seems like, for this grid,
the minimum goes below -1 (which is why this is even a problem - since
the minimum is below -1, it rescales weirdly). But it shouldn't get
below -1 at all. Hm. At least the values from both my numpy and
tensorflow render implementations match, though I'm not sure why
values are going below -1...

Ah I know. It's because the weights themselves are not guaranteed to
be positive. I'm not sure if this is a problem for my past runs, but
at least I'm aware of it now. I should probably repeat the past runs
after and limit them to between 0 and 1 to prevent using negative
weights as combiners - but considering the last network was a regular
grid, I don't think it should be too much of a problem. Maybe it
matters more for this grid where phosphenes are much more likely to
overlap, but I'll have to come back to this. 

I've fixed it now (by that I mean, now the UniqueElectrode grids
normalise between -1 and 1, just like the Electrode grids), but I
guess there's still questionability on how "real-life" like it is. But
at the very least, the nonLinear grid should give values between -1
and 1 like the MNIST (and this was already the case for the
polarRegular grid which clips them between those two values anyway). 

Training time is slightly longer, about 100 seconds an epoch. I think
that's alright for now.

---

Okay, I've retrained the MNIST decoder network with the rescaled
images and with better random inputs (or at least random inputs more
like what it's going to get from the encoder. We'll see how it goes.

** [2019-06-12 Wed]
   
*** Using Softmax for th Encoder Output
:LOGBOOK:
CLOCK: [2019-06-12 Wed 09:30]--[2019-06-12 Wed 10:52] =>  1:22
:END:

| Git commit hash | b0b1e5d2139dae22bcaffce9f229d52bfef8a2b6 | 
| Grid | "nonLinear" (to rename to "rescaling") | 
| Vector size | 12 x 12 = 144 | 

The results don't look terrible, but they're still washed out. I think
I realise the problem now - I should have been using softmax on the
output of the encoder to ensure that the outputs are between 0
and 1. I should probably repeat the previous ones as that might
change the performance. 

I'll run the "nonLinear" interaction grid with as softmax output layer
now and see how it goes.

---

It seems to have gone much better. Since the inputs to the renderer
are now between 0 and 1, that means it looks like other grids. Though,
I think removing the colour lowered the distortion somewhat. I've
tried experimentally with the "nonLinear" grid and it's not too hard
to decode the digit with the brightness rather than colour random
values. 

The training gif:

[[./03-psychophysics/./data/training-intermediate-data/training-gifs/2019-06-12_10-01_evolution_NonLinearInteractionGrid_12-12_64-64.gif]]

And the loss curve:

[[./03-psychophysics/./data/training-intermediate-data/training-graphs/2019-06-12_10-01_loss_NonLinearInteractionGrid_12-12_64-64.png]]

It's much harder to see the granularity on the loss because at the
very start, it's so high.

I think the output is fairly reasonable and I think its encouraging
that the network can still do this even when its available space of
phosphenes is distorted. 

*** Meeting :meeting:

Been continuing on with encoder training with additional improvements
to:
1. MNIST decoder architecture - 2D convolutional layer, larger
   kernels, more garbage data to train on 
2. Encoder training - softmax for the output layer, no hidden layers, 
3. Digit decoder (the stable network part of the encoder), more data
   to train on, one more convolutional layer for better performance

The Polar Regular grid seems to get fairly reasonable results now, so
I am working on extending it to the "non-linear" (maybe needs a better
name, because it's really the /most/ linear of the grids since the
other grids to addition then clipping. Maybe I shold call this the
"rescaling" grid) and irregular grids, and testing this 

I'm still not sure if having more hidden layers on the encoder is
beneficial or not.

- Also, regarding "usefulness" - I think this is interesting, but I'm
  still not confident this will actually get better performance than
  the nonLinear grid with scanning. From what I've felt when testing,
  scanning the image is really helpful, even with poor grids. 


** [2019-06-17 Mon]

*** Preliminary Analysis - Preparation
:LOGBOOK:
CLOCK: [2019-06-17 Mon 08:59]--[2019-06-17 Mon 09:58] =>  0:59
:END:

I think what I'll try to do today is to just get some preliminary
results and analysis for a small hypothesis - that a machine learned
algorithm to statically map digits to simulated phosphenes provides
beter classification accuracy than a brightness-based approach, for a
map of irregular phosphenes. 

I've added a command line flag to digits.py to enable or disable
scanning. I've also made 100% sure that original image vector is
normalised between 0 and 1, which I've had to change because now the
Stimulus class takes 3 dimensional RGB data rather than the grayscale
data.

I've managed to hook the trained encoder and grid up to the digit
experiment successfully now. I had some issues with normalising the
images from the image files and orienting them in the correct
direction (I originally had the up-down flip in the image loading
section, but I have moved it to post-render). 

Here's an example of a command to run the experiment with a
"nonLinear" interaction grid, and pretrained processor, with scanning.

#+BEGIN_SRC sh
python .\digits.py --testing --ncues 10 --ntrials 1 --grid ".\data\training-intermediate-data\training-grids\2019-06-12_10-01_grid_NonLinearInteractionGrid_12-12_64-64.pkl" --encoder ".\data\training-intermediate-data\training-encoders\2019-06-12_10-01_encoder_NonLinearInteractionGrid_12-12_64-64.h5" --processor net --with-scanning
#+END_SRC

I might as well give it a go now I guess. I'm not expecting the
results to be exemplary, but I can just try and see how to analyse the
results I do get.

[2019-06-17 Mon 10:09] 
I quickly also had to normalise the output of the direct process to
between 0 and 1 for the grid. This was not a problem before, but
normalising the RGB image to between -1 and 1 made this a necessary
change as well.

*** Preliminary Analysis - Experiment
:LOGBOOK:
CLOCK: [2019-06-17 Mon 09:58]--[2019-06-17 Mon 10:11] =>  0:13
:END:

Here's what I'll do. I'll do three conditions, with 20 cues per trial
for 10 trials. I'll keep the audio feedback (both correct/incorrect
and the digit class) on - actually, I'm not sure whether I should turn
it off at all for the experiment. 

The three conditions will be:

1. No-scanning, brightness
2. No-scanning, pretrained
3. With-scanning brightness

I think it's pointless testing the pretrained with-scanning condition
because scanning doesn't provide any useful information for the
pretrained encoder. I guess I can try but I'm not sure it will provide
any useful information other than what we already know - the
pretrained encoder can't use scanning.

The grid is the same for all of them - it's got some distortions, but
the "nonLinear" component (i.e. the rescaling of phosphenes to other
phosphenes) is not actually that large. I don't think the results are
going to be particularly interesting. 

Okay, time to run the experiment. 

**** Condition 1
:LOGBOOK:
CLOCK: [2019-06-17 Mon 10:13]--[2019-06-17 Mon 10:18] =>  0:05
:END:

Here's what I'll start with.

#+BEGIN_SRC sh
python .\digits.py --ncues 20 --ntrials 10 --processor direct --grid ".\data\training-intermediate-data\training-grids\2019-06-12_10-01_grid_NonLinearInteractionGrid_12-12_64-64.pkl" 
#+END_SRC

I'll save it as JW. No scanning is the default, so no with-scanning
flag is fine.

It went alright. It was surprisingly hard to tell the difference
between some numbers, like the 2 and 7, or the 9 and 5. 

**** Condition 2
:LOGBOOK:
CLOCK: [2019-06-17 Mon 10:18]--[2019-06-17 Mon 10:23] =>  0:05
:END:

For the next condition:

#+BEGIN_SRC sh
python .\digits.py --ncues 20 --ntrials 10 --processor net --grid ".\data\training-intermediate-data\training-grids\2019-06-12_10-01_grid_NonLinearInteractionGrid_12-12_64-64.pkl" --encoder ".\data\training-intermediate-data\training-encoders\2019-06-12_10-01_encoder_NonLinearInteractionGrid_12-12_64-64.h5" 
#+END_SRC

That was also surprisingly hard. Some were obvious, like the 7 and 2
and 5, but distinguishing the 4, 9, 8 and 3 were extremely difficult.

**** Condition 3
:LOGBOOK:
CLOCK: [2019-06-17 Mon 10:24]--[2019-06-17 Mon 10:29] =>  0:05
:END:

Here's condition 3 with scanning.

#+BEGIN_SRC sh
python .\digits.py --ncues 20 --ntrials 10 --with-scanning --processor direct --grid ".\data\training-intermediate-data\training-grids\2019-06-12_10-01_grid_NonLinearInteractionGrid_12-12_64-64.pkl" 
#+END_SRC

That was much easier than either of the other two. I already know that
the score is going to be much higher. 

Maybe I should actually get rid of the feedback for the "testing"
phase - I guess you could argue that more encouraging feedback may
affect the result. 

*** Preliminary Analysis - Analysis
:LOGBOOK:
CLOCK: [2019-06-17 Mon 11:50]--[2019-06-17 Mon 12:10] =>  0:20
CLOCK: [2019-06-17 Mon 10:30]--[2019-06-17 Mon 11:49] =>  1:19
:END:

Here are the three files, all under the directory 

First, loading the input (running this from /within/ the
=03-psychophysics= directory

#+BEGIN_SRC python :session prelim :noeval
import glob
import json
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix

data_dir = "./data/psychophysics-sessions/participants/"

sessions = glob.glob(data_dir + "*_config.json")

data_sessions = [pd.read_csv(f.replace("_config.json", "_session.csv")) for f in sessions]

data_configs = []
for f in sessions:
    with open(f) as infile:
        data_configs.append(json.load(infile))

data = list(zip(data_configs, data_sessions))
#+END_SRC

First, comparing mean accuracy. 

#+BEGIN_SRC python :noeval :session prelim
fig, axes = plt.subplots(1)

colors = ['tomato', 'seagreen', 'royalblue']

# d[0] corresponds to config, d[1] corresponds to session data
accuracies = [sum(d[1]['digit'] == d[1]['keypress'])/len(d[1]) * 100 for d in data]
labels = [f"{d[0]['PROCESSOR_TYPE']}, {('no scanning', 'with scanning')[d[0]['WITH_SCANNING']]}" for d in data]
          
plt.bar(range(n_data), accuracies, align='center', color=colors, tick_label=labels)
plt.ylim((0,100))
plt.title("Mean Digit Classification Accuracy by Processing Method\n{}".format("JW:2019-06-17_10-01_grid_NonLinearInteractionGrid_12-12_64-64.pkl"))
plt.ylabel("Percentage Accuracy")
plt.axhline(10, color='r', linestyle=':')
#+END_SRC

[[file:./03-psychophysics/data/archive/2019-06-17-mean-digit-classification-accuracy-preliminary.png]]

I'm not surprised since this is also the general impression I got
during the trials. 

Comparing the mean time for each condition: 

#+BEGIN_SRC python :noeval :session prelim
fig, axes = plt.subplots(1)

# d[0] corresponds to config, d[1] corresponds to session data
times = [d[1]['cuetime'].mean() for d in data]
stds = [d[1]['cuetime'].std() for d in data]
          
plt.bar(range(n_data), times, align='center', color=colors, tick_label=labels, yerr=stds)
plt.title("Mean Time to Classification per Cue by Processing Method\n{}".format("JW:2019-06-17_10-01_grid_NonLinearInteractionGrid_12-12_64-64.pkl"))

plt.ylabel("Time to Classification (seconds)")
#+END_SRC

Error bars show standard deviation. 

[[file:./03-psychophysics/data/archive/2019-06-17-mean-classification-time-preliminary.png]]

I'm not too surprised. I'm biased; maybe I should have spent more time
on the direct, no scanning condition.

Plotting the confusion matrices:

#+BEGIN_SRC python :noeval :session prelim
import seaborn as sn

fig, axes = plt.subplots(1, n_data, figsize=(12, 4))

for i in range(n_data):
    sn.heatmap(confusion_matrix(data[i][1]['digit'], data[i][1]['keypress'], labels=range(10)),
               annot=True, 
               ax=axes[i],
               cbar=False)
    axes[i].set(xlabel='Predicted digit', ylabel='True digit', title="Confusion matrix, {}".format(labels[i]))

fig.suptitle("JW:2019-06-17_10-01_grid_NonLinearInteractionGrid_12-12_64-64.pkl")
#+END_SRC

[[file:./03-psychophysics/data/archive/2019-06-17-confusion-matrices.png]]

I think it's rather amusing that I literally never got the 4 right in
the pretrained net processor. Comparing across rows and along columns,
it's also clear that while the digit frequencies were relatively
similar (as they should be), my classification choices were not
uniform - the 9 for the pretrained net classifier, for example, gets
chosen much more frequently than the 4. Perhaps this indicates that
the 4 just looks too much like a 9.

I suppose once I get more participants, I could do a t-test to compare
between specific groups. I don't know if it's meaningful to compare
within an individual, or actually what test I should use. I suppose
whether the digit is correct or not, as a binary variable, could be
modelled with a logistic regression. I'll have to think about that. 

But I'm not surprised with these results. I'm not finished with the
encoder yet, but I guess it's interesting to take this quick detour
back into the experimental zone. 

*** Progress Review
:LOGBOOK:
CLOCK: [2019-06-17 Mon 13:12]--[2019-06-17 Mon 13:26] =>  0:14
CLOCK: [2019-06-17 Mon 12:12]--[2019-06-17 Mon 12:44] =>  0:32
:END:

My biggest dissatisfaction with the current approach is the inability
to scan when using the pretrained CNN processor. Scanning, from my
literature review and also from my observations when doing the
experiment, is really quite helpful to make up for loss of information
from the distortions and gaps in the grid. My initial thought was that
there was a compromise between location-information and
form-information for a digit-recognition task on a grid, and that a
CNN-based decoder would optimise form-information at the cost of
location-information (which may be an acceptable tradeoff for certain
tasks), but now I sort of believe that scanning can compensate for the
lack of form-information because you have an additional, controllable
input -  your position in space. 

It seems like scanning is another channel of information which you can
control. And the assumption that the digit is static in form means
that you can use that channel effectively because any changes must be
due to changes in location, not in form - by scanning, you can take
advantage of these assumptions. I guess, conversely, by removing
scanning and by keeping stationary for a pretrained CNN decoder which
is purely form-based, you can infer that any changes in appearance
must be changes in form, and not in location - but this inference is
much less useful in real life it seems. Most informative information
from a reading perspective, for example, is static in form.

I guess there is one additional advantage of non-scanning - you don't
have to move to get a good picture, and pictures are relatively
stable. This experiment simulates scanning with a mouse, but I was
moving it quite quickly. 

Here's a plot on the first trial of mouse movements (at a rate of
approximately 5Hz, with each measurement corresponding to a dot).

[[file:./03-psychophysics/data/archive/2019-06-17-mouse-movements-direct-with-scanning.png]]

The x and y axis correspond directly to the x and y axis of the
stimulus screen (scale -1 to 1 where -1 is bottom left, 1 is top
right). The line and markers become lighter with time. 

So what should be the direction from here? 

Maybe I will try to get the digit forms to look better. But as I
mentioned, even at the end, this approach has very clear limitations. 
But I still think it would be interesting. And I guess trying to think
about the real-life implications is maybe a bit premature. There's not
really a reason why both couldn't exist. 


*** Using a Regular Grid and Increasing the Number of Electrodes

I'm going to increase the number of electrodes to 24 x 24 and see if
that gets better digits, and I'll just use the regular grid this time
too. I'll keep the rendered grid at 64 x 64 pixels. 

I think the =phosphenes.py= file could use a good refactor. The grids
could all inherit from a single class, which would reduce the amount
of repetition in the code at the moment. But I'll do that later, just
making a note now. 

Generating the training data for the MNIST digit decoder is...quite
time consuming. It's been at least 5 minutes and still going. I guess
that makes sense considering the garbage is now 4 times as large. 

I ended up interrupting the kernel, I think I'll decrease the amount
of garbage data I need to make.

I've reduced the Gaussian filter PBASE paramter from 2 to 1, because
it was bleaching the PolarRegularGrid now with the clipping done. 

I've reduced the amount of garbage by a factor of 10, back to where I
had it before. It took about 1 minute to generate the data. I'm not
sure if this might reduce the quality of the results though.

Actually, nevermind that, I decided to try again with the full garbage
data. I don't really want to compromise on that to be honest, as it's
that which is keeping the output from looking very poor. - actually,
I'll do a compromise, I'll do 2 times, not 10 times.  It took about 1
minute as well, I think it's a reasonable compromise...I think I'd
prefer more data of course, but I'll leave it for now.

I'm also going to try adding another layer to the decoder - a 32x32
kernel, which is half the original image. We'll see if that helps. 

Training with this extra layer is going quite slowly - about 7 minutes
per epoch - but hopefully it will be worth it. I just wonder whether
having a large kernel may be better in providing more "holistic"
classification, rather than granular lines. 

While I'm waiting for the MNIST decoder to train, I think I need to
specify out the questions for my project again:

1. Can a pretrained image encoding network produce phosphene-versions
   of digits that improve human classification accuracy compared to
   static direct encodings?
2. If so, what encoding network architecture and training method best improves the human
   classification accuracy of the resulting phosphene digits?
3. How does a pretrained image encoding network for static digits
   compare to a direct encoding which allows scanning movements? 
4. In what map distortion settings can this image encoding network be
   useful?

I think the fourth question is probably the most real-life
applicable. I think what I should aim to demonstrate is that these
methods are more robust against distortions, not necessarily that they
are better than direct encoding for all cases. I doubt it will be
better than a direct encoding for cases where the distortion is
minimal, but the full range of distortions of phosphenes are not yet
well known. Being able to demonstrate robustness to map distortions
would be important in such a case. 

But how exactly does one demonstrate robustness? One could compare
accuracies across different distortions, but you would have to attempt
to prove equivalence, which I think may be quite hard. 

Back to the MNIST decoder training - it seems the accuracy isn't quite
as good, though to be honest, that might just be because I reduced the
amount of garbage data (which would be fairly easy to separate). (That
being said, it's not terrible - 97-98% at the start of epoch 5). 

Training is *much* slower with the 24x24 grid. It takes at least a few
seconds to do a single batch. Maybe I should stick with the 12 x 12
grid. Though it could also be because of the extra MNIST decoder layer
(though I sort of doubt it - it's not being trained). Hm. I probably
should have tried just changing one of those things first, but to be
honest, this is purely just a "see if it gets better results with more
pixels."

But I think it's taking a bit too long. I think I'll reduce it down to
a 16 x 16 grid instead. 

I'm going to repeat what I did, but with a 16 x 16 regular grid this time.

*** 16 x 16 Regular Grid, Extra MNIST Layer. 
:LOGBOOK:
CLOCK: [2019-06-17 Mon 15:10]--[2019-06-17 Mon 16:11] =>  1:01
:END:

| Git Commit Hash | 77573c2e7d3d27e9ce72d9fe0dc55f5fdd55172d|
| Grid | RegularGrid|
| Vector size | 16 x 16 = 256 |

Training MNIST again. I'm trying to think about what might be a good
way to improve how the digit representations look. Loss function is
what's guiding it, maybe there are better approaches to what I'm
currently attempting. 

Training the encoder is slightly faster than the 24x24 round. It's
bearable. 

I got a rather weird error message regarding unable to reshape and it
looked like it was because the last batch had 0 images. I'm not
entirely sure why that was the case or why it only occurs for the
last loop, but I've set a conditional to avoid this problem now.

It seems to be training very slowly though. The encoder, at
initialisation, outputs very small values which means the image is
very dark. I've upped the learning rate to 1e-1, we'll see how that
goes. 

Training is really poor for this run. I'm not entirely sure why. I
think I'll try upping the learning rate up to 1. 

But even so, it's not training very well at all. Maybe it's because
the phosphenes are so much more spread out, or because it starts with
a rather poor configuration. I'm not entirely sure. 

I think I'd also like to just spend some time doing some extra
reading.

I think I'll stop this one here. I'll retry the polar grid with 16 x
16 electrodes instead. 

*** 16 x 16 Polar Grid, Extra MNIST Layer

| Git Commit Hash | 292c5be2b26976c70e82cc849e32a7612bdfd379|
| Grid | PolarRegularGrid | 
| Vector size | 16 x 16 = 256 |

I sort of think that perhaps reducing the gradient of blur might be a
significant factor. 

[2019-06-18 Tue 08:58]
I left it running while I went to work yesterday for 100 epochs, and
it didn't work very well at all. The loss function didn't descend,
which in some respects may be good. 

Not very happy with it. Kept for posterity, gif name at
=2019-06-17_16-09_evolution_PolarRegularGrid_16-16_64-64.gif=.

I sort of wonder whether this might also have to do with the fact that
I changed the last layer to softmax. I think that would be the most
significant change, and that would reduce the ability for some
contours to merge. 

** [2019-06-18 Tue]

*** 12 x 12 Polar Grid, Extra MNIST Layer
:LOGBOOK:
CLOCK: [2019-06-18 Tue 09:03]--[2019-06-18 Tue 11:11] =>  2:08
:END:

| Git Commit Hash | 7cef1cf8540624a3cfcefe948e44696a1f50744a|
| Grid | PolarRegularGrid | 
| Vector size | 12 x 12 = 144 |

I'm going to do what I should have probably done beforehand - 12 x 12
Poalr Grid with the Extra MNIST Layer, and also using the softmax
output I had done prior. 

I'll leave it running in the background while I try to develop a more
concrete plan.

Just a side note - some ideas which might be good:
- Stochastic noise to either the decoder, or the labels. 

Actually, to be honest, the softmax may not be appropriate, at
all. Actually, I made a big mistake with that I think. There's
absolutely no reason the outputs should add to one, and that would
explain why everything was so dim for the two previous attemps. Argh.

I'll use sigmoid instead, that accomplishes what I need it to do
(activation between 0 and 1). But I think that means I shouldn't take
last two runs' failure too much to heart. 

I think something that's becoming clear is that some intrinsic
properties of the renderer are quite important in training the
encoder. For example, when the renderer can no longer "bleach" the
digits (and the phosphenes therefore have a set, maximum brightness),
the results are markedly worse than when it can. 

What I think would be important and very useful would be to determine
a way to deal with this issue - style transfer, for example, could
copy the style of random renders onto the mnist digits, and train
against that. More networks. But that might give slightly better
results than trying to train directly with MNIST. 

[2019-06-18 Tue 11:11]
I finished the run - the output really doesn't change much after the
10th or so epoch. The output isn't very nice, but it's definitely
better than the last two attempts when I was using softmax. 

Training gif:

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-18_09-01_evolution_PolarRegularGrid_12-12_64-64.gif]]

And loss curve:


[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-18_09-01_loss_PolarRegularGrid_12-12_64-64.png]]

The loss didn't descend to quite as small a value as some of the
previous runs.

*** Plan

For the experimental portion, I think the experiment I've got so far
is fine. I can determine the digit classification accuracy, timings
and (with scanning), mouse movements of participants for
analysis. With multiple participants, I can (I think) run some
statistical tests to determine significance.  

So I think my experimental portion will be comparing processors and
renderers. I guess that's pretty much what I described in the original
project outline.

Processors:
- Brightness
- Per-renderer trained neural network

Renderers:
- Polar Regular Half-Grid
- Deterministic Irregularly Shaped + Size + Brightness Half-Grid (but constant map)
- Deterministic Non-Summative grid where brightness of phosphenes is
  conditional on the brightness of other phosphenes

That gives 2 x 3 = 6 total conditions to test, and I think it's
feasible to test both processors in one subject (so just changing the
renderers between subjects), so 9 participants would give three
repeats of each. 

I think that's fine for the experimental part. 

I think it would be very interesting if I could incorporate a
time-dependent renderer (or at least a renderer that depends on the
previous input vector in some way). But I'll have to think about that
and whether I have time. 

On the theoretical front, I think it would be interesting to quantify
the losses and visualise the kernels. And to describe the architecture
of the network and the training process, though I think there probably
needs to be some theoretical discussion behind the rationale for
what's been chosen (which I will need to work on). 

*** Object Detection(?)

I've made a new directory, =04-detection= and a new conda environment
(activating with =conda activate detection=) to test out the
pretrained object detection models. I know this is a bit of a detour,
but I am becoming increasingly convinced that maybe a simple,
"highlight-this-object" approach is potentially more useful than the
identity-focused approach I came to. 

Importantly, it removes noisy information and is extremely simple to
understand and interpret. And while I previously thought that the lack
of depth encoding in this approach would make it useless, I think
parallax motion and scanning could actually compensate fairly well.

Clearly, this is not useful for certain tasks (e.g. reading), where
there must be knowledge of multiple identities at once. But I think
there is a real use case for such a tool, whereas for the digit
recognition task, I am not so sure. 

Ah, but I should just focus on the current task, even if it's less
useful than a detection task. I think they're orthogonal - I can much
more easily think of a use case for an object detection task that
would be hard to replace with another modality, but that doesn't
necessarily mean it's better. They're just focusing on different
approaches. 

*** 16 x 16 Regular Grid, Sigmoid Activation
:LOGBOOK:
CLOCK: [2019-06-18 Tue 11:31]--[2019-06-18 Tue 12:00] =>  0:29
:END:

| Git Commit Hash | e011de74dc8d0a59602e2073d8222b147c05481|
| Grid | Regular Grid | 
| Vector size | 16 x 16 = 256 | 

I'm going to try this again now that I figured out the problem with
softmax. I'm reusing the [[./03-psychophysics/data/training-intermediate-data/training-decoders/2019-06-17_15-30_mnist_decoder_RegularGrid_16-16_64-64.h5][MNIST decoder I trained for this grid
previously.]]

[2019-06-18 Tue 11:47]
It doesn't appear to be working very well either. I have an inkling
it's because of the max brightness limit, it definitely doesn't appear
very uniform (with much higher brightness near the edges where the
Gaussian blur is most significant). I don't really have much hope
after the first 10 or so epochs. I'll keep it running but I don't
expect to reach much better. 

[2019-06-18 Tue 12:24]
Loss decreased very well, but the results look very poor. 

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-18_11-28_loss_RegularGrid_16-16_64-64.png]]

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-18_11-28_evolution_RegularGrid_16-16_64-64.gif]]

Maybe I should just try a completely regular grid i.e. absolutely no
size distortions whatsoever. 

*** Refactoring =phosphenes.py=
:LOGBOOK:
CLOCK: [2019-06-18 Tue 13:16]--[2019-06-18 Tue 15:34] =>  2:18
:END:

I spent some time refactoring =phosphenes.py= to:

1) Make sure values are scaled so that the max brightness is always
   white
2) Change how the radius is spread is done, so peripheral phosphenes
   are smaller and less aggressively spread
3) Increase the rescaling ( by squaring and dividing by the square of
   the max) of the rescaling grid

I also cleaned it up and made it more idiomatic so it is easier to
extend.

I've also updated =digits.py= accordingly.

I think this will make it much easier to test, now that the
=phosphenes.py= parameters are much more modular.

*** Cartesian 12 x 12
:LOGBOOK:
CLOCK: [2019-06-18 Tue 15:51]--[2019-06-18 Tue 16:50] =>  0:59
:END:

| Git Commit Hash |  99ef69990e3f9bc3c7c7859a7fc7ef1d24a5fdff |
| Grid | Cartesian Grid | 
| Vector size | 12 x 12 = 144 |

Now that =phosphenes.py= has been refactored and hopefully the
brightness is reasonable for each phosphene, I'm going to try the
cartesian grid again. Fingers crossed for this run. 

I've also reduced the learning rate back to 1e-2. I think the
difficulty with a high learning rate is that, while it may get
something faster, it 1) may be more local, and 2) may have difficulty
fine-tuning at the end. 

[2019-06-18 Tue 15:57]
The loss is going down nicely, but the images /still/ look very
poor. I think it may be time to reconsider my loss function. 

I stopped it at 7 epochs because I didn't think it would get any
better. Loss got below 0.1, but the results were completely
unintelligible.

[2019-06-18 Tue 16:08]
I'm going to quickly try putting the sum of the vector as part of the
loss, so hopefully it will try to only preserve the bare
minimum... hopefully.

| Git Commit Hash | af39260c9a2ca0bf14b55e66c7868406b399e1a1|

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-18_15-18_evolution_CartesianGrid_12-12_64-64.gif]]

While that's running.. what else is causing the outputs to not look
like digits?
1. Non-continuous connections between the digits
2. Disjoint areas 
3. Low contrast

I just realised penalising on the total sum of the image is not a good
idea, that'll just lower the contrast even more. Instead, I should be
trying to optimise for the standard deviation of values, or some other
way to try and optimise for 0s and 1s. 

I stopped it after 22 epochs. I think the idea was along the right
longs, but not quite the right one. I'll try using the reciprocal of
the standard deviation of the encoding vector instead and I'll reduce
the learning rate again down to 5e-3. It'll be slower to train, that's
for sure, but I'll see how it goes. 


*** Cartesian 12x12, reciprocal of std loss

Not sure how it'll go with the much slower learning rate, but we'll
see. 

[2019-06-18 Tue 21:52]
After 50 epochs, it didn't really change much, but I could see the
loss was still in the progress of climbing down.

I'll retry the polar grid.

*** Polar 12 x 12, reciprocal of std loss

| Git commit hash | 78d551f1abac281acd17e210ef56fe84372de075 |
| Grid | PolarGrid| 
| Vector size | 12 x 12 = 144 |

I'll make the decoder use the full 60000 garbage images again. I'll
make the learning rate 5e-3, between what I'd tried before, and go for
100 epochs overnight.

[2019-06-19 Wed 09:22]
This run wasn't particularly successful. Change stagnates after the
first ten or so epochs again. 

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-18_22-17_loss_PolarGrid_12-12_64-64.png]]


[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-18_22-17_evolution_PolarGrid_12-12_64-64.gif]]

** [2019-06-19 Wed]

*** Meeting

Progress: 
1. Experimental + analysis plan for discussion
2. Continuing to experiment with generation algorithms
   
**** Plan

Status:
- [X] Experimental task script.
- [X] Per-phosphene map CNN-based encoder training.
- [ ] High discriminability (atm gives similar performance to
  local-brightness-based encoder for experimental run on myself,
  though I am clearly biased)

Ideally, the plan should be relatively stable now. The only thing
that's left is to "slot in" the pretrained net, which I am continuing
to work on. I've tested the pretrained net in the experiment and
functionally it works fine, it's a matter of improving discriminability.

***** Experimental conditions

|------------+------------------+------------------------------------------------------------------------|
| Conditions | Processor        | Renderer                                                               |
|------------+------------------+------------------------------------------------------------------------|
| 1A         | Local brightness | 144-phosphene polar regular right-half grid, no scanning               |
| 1B         | Pretrained net   | 144-phosphene polar regular right-half grid, no scanning               |
| 2A         | Local brightness | 144-phosphene polar irregular right-half grid, no scanning             |
| 2B         | Pretrained net   | 144-phosphene polar irregular right-half grid, no scanning             |
| 3A         | Local brightness | 144-phosphene polar irregular conditional right-half grid, no scanning |
| 3C         | Pretrained net   | 144-phosphene polar irregular conditoinal right-half grid, no scanning |
|------------+------------------+------------------------------------------------------------------------|

***** Experimental Process

Aiming for minimum 6 participants.

On recruitment:

1. Allocate to one of groups 1, 2, or 3 (both A and B
   portions are done by one participant.
   This gives 2 people per numerical group.
   Of the 2 people, set one to start with condition A and set one to
   start with condition B. 
2. Schedule a 1 hour session block with rest between A and B. 

During experiment session:   

1. Explain nature of experiment and gain consent.
2. Explain the task - along the lines of "You will be shown simulated
   images that represent digits. We would like you to press the digit
   you think it is. Immediately afterwards, you'll get feedback on
   whether you were correct or not, and what the actual digit is. This
   will be repeated 20 times, for 25 trials and should take about 15
   minutes."
3. Allow a trial run (should I record form this?) of one trial to let
   them be familiar with the experiment.
4. Start the experiment script with =--ntrials 25 --ncues 20= and
   appropriate processor and grid parameters for experimental group,
   with either condition A or B.
   
   This will produce 500 datapoints for each condition for each
   participant, or 1000 data points for each condition across
   participants. 

   Digits for the local-brightness based decoder will
   space-invariant...maybe. 
   - On one hand, if they are in different positions, then could argue
     that people can't train to recognise them
   - On the other hand, if they are in the same position, then could
     argue that it's just the particular position which is bad

5. At the end of the experiment, allow the paticipant to take a rest.
6. Start the experiment again with the condition A or B that was not
   tested before. 
7. On conclusion, debrief. 

***** Analysis and Discussion for Thesis

Main question - *is a pretrained net more /robust/ against different
renderers than a brightness-based encoder?* (or more adequately able
to preserve classification accuracy when distortions are introduced to
the renderer). 

The intended tie-back to CVPs is that there are many distortions to
in-vivo phosphene percepts, many of which we already know about
(e.g. size and brightness), but many others of which are currently
unpredictable or uncontrollable (precise location, shape), or are
currently unknown or not-well-established (interactions of multiple
phosphenes). Being able to find methods which are capable to being
/robust/ to distortions /could/ be useful to cope with unpredictable
or unknown percept distortions. 

At the moment, the training is confined to a simulated setting. I
originally thought that training could be done during training, but
the huge data requirements are probably not feasible at the moment.

- What was the accuracy /between/ renderers? :: Previously, I showed
     the basic analysis of this on an earlier prototype. 

     Outputs:
        1. Bar chart of mean accuracy (I'm not sure what error bar would
            exactly be meaningful for this graphic).
        2. ANOVA between the three renderer groups (n = 2 participants for each
            renderer) for difference in mean accuracy

- What was the accuracy /between/ processors? :: Very similar to the
     previous, but comparing between processors instead of 

- What was the difference in timing /between/ renderers? ::
     similar. To inform discussion whether difference in accuracy may
     be attributable to differences in time spent.

- What was the difference in timing /between/ processors? ::
     similar. To inform discussion whether difference in accuracy may
     be attributable to differences in time spent.
     
- For each condition, what digits were often confused? :: To inform
     discussion on what digits were easy or hard to discriminate and
     what particular use cases of a different encoder based on
     features of these digits may be.

     Outputs:
     1. Confusion matrix per condition. 

- Which processor was more robust against the three renderers? :: I am
     not yet sure what the best analysis for this would be or how to
     test the significance of this. 

     A simple measure would be a measure of spread of the
     classification accuracies for each processor (e.g. variance or
     standard deviation), but for only three renderers, I do not know
     how meaningful this would be. 
     
     The previous analyses of accuracy and confusion could inform the
     discussion of this point. 
        
I would also like to incorporate some non-experimental analysis of the
performance of the encoder training.

- Comparison of encoder training architectures :: I've tried many, and
     am continuing to try many. I would like to incorporate some
     visualisation of the various encoder architectures I've tried,
     though maybe only a brief discussion. I can visualise the end
     result after n-epochs and the loss curve.
     
- Assessment of "intrinsic" discriminability between digits for each processor and renderer :: I think this would be useful to inform
  the discussion on the limitations of the non-experimental training
     and why the non-experimental training differs from experimental
     results, and potentially how these could be bridged.
     
- Kernels for adversarial decoder :: I've done a few of these, and I
     know it's pretty abstract, but I think it may be useful to know
     what features the decoder is decoding against in the first
     place. I haven't fully investigated this yet, but I believe that
     this is crucial to the training, and is this is currently wha
     the optimising against. 

I believe these would contribute to a discussion in the thesis of what
the intended basis for this project was and, depending on the
experimental results, why or why it does not translate to an
experimental setting.

***** Timeline

|----------------+------------------------------------------|
| Week Starting  | Task                                     |
|----------------+------------------------------------------|
| July 1st       | Begin recruiting participants            |
| July 8th       | Run 2 participants, continue recruitment |
| July 15th      | Assess first batch                       |
| July 22nd      | Run 2 participants, continue recruitment |
| July 29th      | Run 2 participants                       |
| August 5th     | Conduct analysis                         |
| August 12th    | If necessary, run more participants      |
| August 19th    |                                          |
| August 26th    |                                          |
| September 2nd  | Start writing thesis                     |
| September 9th  |                                          |
| September 16th |                                          |
| September 23rd |                                          |
| September 30th |                                          |
| October 7th    |                                          |
| October 14th   |                                          |
| October 21st   |                                          |
| October 28th   | Thesis and Faculty Oral + Poster Due     |
|----------------+------------------------------------------|

***** Potential Criticisms

1. No incorporation of what is "lost" with pretrained net - i.e. the
   inability to scan / the static nature of render. This is intrinsic
   to the processing method of the net, and I imagine this will be a
   very strong criticism of why this particluar experimental method 
   is not readily transferrable to a real-life situation. Scanning
   provides an extra, user-controlled channel of information that is
   really useful to increase the knowledge about the data, but is not
   assessed in this methodology.
   
2. Spatial position of digits for the brightness encoder (discussed
   above) - this comes down to the question of "what is a fair
   comparison". 

3. Lack of "control" - e.g. completely random render, or a completely
   clean render. You would /think/ that the former would have 10%
   accuracy, and the latter close to 100% accuracy but I don't know if
   I need to explicitly prove that. I would think not. 

4. Small sample size. 
   
5. Confined to simulated setting. Incorporating training data from
   participants would be useful, but from my results so far, the data
   requirements are absolutely massive (10,000 digits per epoch, and
   it takes about 5-10 or so epochs, from what I have so far, until it
   is relatively stable). It's also a question of how to do the
   backpropogation properly, but I don't think it's feasible to
   collect the data to do so anyway, so I will confine the training to
   prior to the experiment (though I do believe there is some novelty
   with this approach, as the training takes the specific renderer as
   an input and trains /for/ that renderer). 
  
6. This isn't a criticism, but I sort of see this as confronting two
   aspects. 
   1) What training architecture produces the best human
      discriminability, and
   2) Can a training architecture improve upon a brightness-based
      encoder on a certain dimension (e.g. accuracy, robustness)
   Number 2 is dependent on 1, but most of the experimental discussion
   (at least as it stands) will be resting on 2. The analysis of 2 is
   very basic (comparing accuracies). I think I would prefer to have
   further discussion on 1 and actually for the emphasis to mostly be
   on 1 since I feel that is the more interesting aspect.
   
7. Eye movements. Since phosphenes move with eye movements, peripheral
   phosphenes in this experiment may be interpreted differently since 
   people can fixate directly on them. 

   

***  12 x 12 Grid Polar Regular Grid, Architecture of decoder change.

| Git commit hash |               |
| Grid            | Polar Regular |
| Vector size     | 12 x 12 = 144 |

I'll go back to a decoder with smaller initial kernels and see how
that goes. 

I think I need to remember that the /primary/ goal of the decoder is
not actually decoding accuracy - it's more about learning useful
features and discarding poor features. 

I've changed the architecture:
- Leaky ReLU instead of ReLU (I wonder if perhaps the negative values
  of the input image could be impacting performance)
- Many more filters, but only two convolutional layers
- Dropout after both layers

We'll see how it goes.

NB: I attempted to put =tf.nn.leaky_relu= as the =activation= kwarg to
the Keras layers API, but when loading the model, it threw an error
saying =leaky_relu= was not recognised. I've split it into a separate
layer - maybe this is because of the Tensorflow 2.0 API changing a few
things, though I'm not quite sure. 

[2019-06-19 Wed 14:22]
Like before, it's pretty stagnant after 10 epochs. Discriminability is
still an important issue. I know MNIST is supposed to be the arbiter
of discriminability here, but maybe something greater is needed. It
could be something easy, like the direct difference between intensity
values of the resulting image. Or perhaps a quick metric would be the
difference between intensity values of the input vector. I might try
that. 

I stopped the current run after 40 epochs, it wasn't changing much
(although the loss was still gradually decreasing). 

[[./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-19_13-01_loss_PolarGrid_12-12_64-64.png]]

[[./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-19_13-01_evolution_PolarGrid_12-12_64-64.gif]]

*** Polar Regular Grid, Many Many more Kernels
:LOGBOOK:
CLOCK: [2019-06-19 Wed 14:35]--[2019-06-19 Wed 15:26] =>  0:51
:END:

| Git commit hash |  70cbd1809681fb6ffd12db6c6d7e75e370a4b3ae                  |
| Grid            | Polar Regular Grid |
| Vector size     | 12 x 12 = 144      |
| Learning rate   | 1e-2               |
| Loss            | MNIST only         |
| Decoder         | 64x8x8 -> 128x4x4  |

I sort of wonder whether the number of kernels is worth increasing
even more. Perhaps this would increase the chance of having kernels
that detect garbage features and would increase the decoder accuracy
for garbage. 

I've added 64 8x8 kernels to the first convolutional layer and 128 4x4
kernels to the second convolutional layer. It's going to take much
longer to train, but hopefully it is worth it.

A paper to read based on multiclass, conditional GANs:

[[https://arxiv.org/pdf/1806.07751.pdf]]

I'm wondering whether it might be worth separating the GAN from the
decoder network - i.e. just attempting to generate conditional upon
digits for a particular map renderer. 

The MNIST training takes around 5 minutes per epoch. The resulting
kernels are interesting - there are some very clear ones which look at
horizontal, vertical or angled lines, as well as loops. Many of them
appear as noise, which I am hoping help exclude garbage. 

For this run, I'm taking out the standard deviation part in the loss
function. I'd rather just see how it goes without it first. 

Also, I'm going to have to think about why the encoder always starts
off with a really bright image. I'm not quite sure yet. 

After 5 epochs, I have to say...I'm pretty impressed. It actually is
starting to look like MNIST digits. So perhaps it is indeed the number
of kernels which is a deciding factor. Maybe I should attempt to
quantify this.

I'm going to stop it at 20 epochs and reintroduce the standard
deviation aspect. I'd like to see if it helps with the background
suppression.

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-19_14-32_loss_PolarGrid_12-12_64-64.png]]

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-19_14-32_evolution_PolarGrid_12-12_64-64.gif]]


*** Polar Regular Grid, Slower Learning Rate and reciprocal of standard deviation contribution to loss

| Git commit hash | 70cbd1809681fb6ffd12db6c6d7e75e370a4b3ae |
| Grid            | Polar Regular Grid                       |
| Vector size     | 12 x 12 = 144                            |
| Learning rate   | 5e-3                                     |
| Loss            | MNIST + 1 / standard deviation           |
| Decoder         | 64x8x8 -> 128x4x4                        |

I'm starting to think that perhaps it would indeed be a good idea to
separate the generative network out and test it separately, then use
participant data to test the encodings and see which ones
work. That way there are more candidates for the digit representations
to choose from, rather than being stuck with whatever the encoder
eventually ended up with. 

[2019-06-19 Wed 15:38]
After 13 epochs, it doesn't really appear to be getting much
darker. Maybe I need a better metric for the loss function to try and
ensure there is  high contrast. I'm imagining something like a sine
function with width 2 units, then find the sum. So the value 0.5 is
the highest value, and 0 and 1 both give a 0 value. There clearly
needs to be a constant applied to it to scale it so it doesn't
overpower the MNIST decoder though, especially since this will be
pretty much 0 at the start when the encoder gives very bright values.
The downside is that this should probably be applied to the simulated
render, not the input vector, which will probably make training
slower. I'll have a try I think, after this run is over.

So at the moment, I think there seem to be two strong possible
candidates for further exploration:
- Increasing the number of filter kernels
- Incorporating a better loss function.

I decided to stop this after 21 epochs, where it remained fairly stable.

[[file:./03-psychophysics/./data/training-intermediate-data/training-graphs/2019-06-19_15-25_loss_PolarGrid_12-12_64-64.png]]

[[file:./03-psychophysics/./data/training-intermediate-data/training-gifs/2019-06-19_15-25_evolution_PolarGrid_12-12_64-64.gif]]

*** Polar Regular Grid, Mapped sine contribution to loss and more kernel filters. 

| Git commit hash |649076929c92673b81e4da21ae5040d0354397e4                                            |
| Grid            | Polar Regular Grid                         |
| Vector size     | 12 x 12 = 144                              |
| Learning rate   | 5e-3                                       |
| Loss            | MNIST + map sin(x*pi) over image, then average |
| Decoder         | 128x8x8 -> 128x2x2                         |

With this new MNIST decoder of many more kernels, it takes about 10
minutes per epoch. Decoding accuracy is also worse, though I don't
know if that's necessarily something to worry about...((though that may
also be because I reduced the kernel size for the second layer). I
mean, could there be such a thing as "too" flexible for the decoder?
Especially given that the primary task is not about decoding per se,
but about discrimination with garbage first and foremost, and then
amongst each other.

[2019-06-19 Wed 16:55]
The decoder is one large file - 17MB. Yikes. Not sure if I should just
gitignore it for now.

I've made the constant multiplier to the sin mapped over the encoded
images to be 1 over the (batch size multiplied by number of pixels in
the image) i.e. simply the average. The sin mapped sum will simply be
the average of that function over all the pixels.

Each epoch is approximately 80 seconds, so not too bad.

[2019-06-20 Thu 09:15]
After hvaing let it run overnight, the digits look pretty similar to
before, except I'm still not getting the contrast I want, and the loss
is actually negative?! That makes me think I didn't use the sine
function correctly. Ah...and I think I know why. I'm using the encoded
images so the output is between -1 and 1...gr. I was still thinking
along the lines of using the encoded values which are between 0
and 1. So that would explain a lot. I should rerun this with the
cosine function, that would be very easy. 

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-19_15-48_loss_PolarGrid_12-12_64-64.png]]

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-19_15-48_evolution_PolarGrid_12-12_64-64.gif]]

I think I should also try to start with non-bleached images. I have to
figure out why they're bleached though.

Just out of interest, here are the kernels for the MNIST decoder for
this run.

[[./03-psychophysics/data/archive/2019-06-20-kernels-polar-regular-128.png]]

** [2019-06-20 Thu]

*** Adding Cosine to the Loss Function Instead of Sine
:LOGBOOK:
CLOCK: [2019-06-20 Thu 09:50]--[2019-06-20 Thu 11:09] =>  1:19
CLOCK: [2019-06-20 Thu 09:40]--[2019-06-20 Thu 09:50] =>  0:10
:END:

| Git commit hash | 0dce5db96ec538c7f31adc4ac450f4958ddad966|
| Grid            | Polar Regular Grid                         |
| Vector size     | 12 x 12 = 144                              |
| Learning rate   | 2e-3                                     |
| Loss            | MNIST + map cos(pi/2 * x) over image, then average |
| Decoder         | 128x8x8 -> 128x2x2                         |

It seems that the bleaching is simply because of the additive and
clipping effect. When the encoder starts, the values are mostly 0.5 -
and when I decrease all those values by a scale of 10, the image gets
dimmer. I don't think it's necessarily something to worry about at
this point.

Time to run this with cosine in the loss function now - hopefully
there's a bit more distinction between the digit and the
background. Then I'll try and concentrate back on the forms again.

I reckon I'll try a slower learning rate too - clearly, there's not
much change in later epochs, so I think I can afford it. 

[2019-06-20 Thu 10:59]
It's at the 34th epoch and it still doesn't seem to be getting enough
contrast. Not sure why though. I might try upping the learning rate
quickly and increasing the contribution of cosine. 

[[file:./03-psychophysics//data/training-intermediate-data/training-graphs/2019-06-20_09-33_loss_PolarGrid_12-12_64-64.png]]


[[file:./03-psychophysics//data/training-intermediate-data/training-gifs/2019-06-20_09-33_evolution_PolarGrid_12-12_64-64.gif]]

*** Increasing the contribution of cosine to the loss function by factor of 1000
:LOGBOOK:
CLOCK: [2019-06-20 Thu 11:09]--[2019-06-20 Thu 11:42] =>  0:33
:END:


| Git commit hash | 2fbb829503359e5cb8b976aa76b96282e7cabb9f               |
| Grid            | Polar Regular Grid                                     |
| Vector size     | 12 x 12 = 144                                          |
| Learning rate   | 1e-2                                                   |
| Loss            | MNIST + 1000 * average of map cos(pi/2 * x) over image |
| Decoder         | 128x8x8 -> 128x2x2                                     |

I think it's very interesting that the digits the network comes up
with are very similar between the different loss functions and
learning rates. Probably a consequence of the decoder. I should
definitely separate out the generative network and try and get more
candidate renders.

[2019-06-20 Thu 11:16] 
Frustratingly, the contrast is still not very
good (I initially tried with 10 times the average cosine). It looks
almost exactly the same as the others. Maybe I'll try absolutely
blasting the contribution of cosine...just to see what it gets.  (I
guess it's understandable though, over half the image is guaranteed to
be -1, making the average probably very low...)

[2019-06-20 Thu 11:40]
This time, it achieved what I wanted, I think I need to make it less
aggressive, but at least I know this is usable.

[[./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-20_11-03_loss_PolarGrid_12-12_64-64.png]]

[[./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-20_11-03_evolution_PolarGrid_12-12_64-64.gif]]

*** Increasing contribution of cosine to the loss function by factor of 100
:LOGBOOK:
CLOCK: [2019-06-20 Thu 11:43]--[2019-06-20 Thu 13:54] =>  2:11
:END:

| Git commit hash |a3bcdf05839d5bf509368d706527c26e20a6f905 |
| Grid            | Polar Regular Grid                                     |
| Vector size     | 12 x 12 = 144                                          |
| Learning rate   | 1e-2                                                   |
| Loss            | MNIST + 50 * average of map cos(pi/2 * x) over image |
| Decoder         | 128x8x8 -> 128x2x2                                     |

Similar to before, but by 100 instead of 1000 - hopefully during the
earlier generations, the contribution will be less so the decoder can
steer the network towards forms rather than trying to optimise for the
cosine straightaway.

[2019-06-20 Thu 12:59]
100 was interesting. It started off like the ones prior to
incorporating the cosing, but after about 10 or so epochs, it started
to "bleed" the brightness (so the digits became larger and more
blob-like) and gradually the greys turned to black. I'm going to
repeat with 50 instead of 100 because I don't think the 100 results
were very compelling, but I think it might be interesting to see what
happens if the cosine contribution is delayed even further. 

[2019-06-20 Thu 13:52]
I've stopped it at 29 epochs, it doesn't seem to be changing
much. Still bleeds out a little too much, but I think it's a step in
the right direction, maybe.

[[./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-20_13-15_loss_PolarGrid_12-12_64-64.png]]

[[./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-20_13-15_evolution_PolarGrid_12-12_64-64.gif]]

*** Adding an extra layer to the generative network

| Git commit hash | |
| Grid | Polar Regular Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | 1e-3 | 
| Loss | MNIST + 10 * average of map(cos(pi/2 * x)) over image | 
| Decoder | 128x8x8 -> 128x2x2

I'm rather curious what will happen if I try to add an extra layer to
the network. I don't think it will end up with better output, but I
don't see any harm in trying.

[2019-06-20 Thu 13:58]
After the first epoch - woah, it really changed fast. I think that
might indicate the learning rate needs to be much slower with this
extra layer added. (currently 1e-2). I'm going to rerun this with 1e-3
as the loss instead.

[2019-06-20 Thu 14:09]
I think this one's not working, they all look the same. I'm stopping
quite early at 7 epochs, but it doesn't seem to be changing much.

[[./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-20_13-55_loss_PolarGrid_12-12_64-64.png]]

[[./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-20_13-55_evolution_PolarGrid_12-12_64-64.gif]]


*** Using many, and larger, kernels on the MNIST decoder. 

| Git commit hash |6389889b78625c940679cf4e066585ded4efb268 |
| Grid | Polar Regular Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | 5e-3 | 
| Loss | MNIST + 10 * average cos (as before) | 
| Decoder | 128x16x16 -> 64x8x8 -> 32x4x4

So it seems that what has made the most difference so far
is the number of filter kernels in the MNIST decoder (or perhaps more
generally, the MNIST decoder architecture). So I'll change that up.

To speed things up a little too, I'm also going to reduce the number
of phosphenes/electrodes. The outputs will probably be more limited,
but I'd like to see how it fares anyway. 

[2019-06-20 Thu 15:18]
Nevermind, I attempted to reduce it down to 8, but the results were
indecipherable. I'll just continue with 12 for now.

[2019-06-21 Fri 09:14]
Results are poor but the loss gets down quite low. I really need to
think about why that is. 

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-20_15-18_loss_PolarGrid_12-12_64-64.png]]

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-20_15-18_evolution_PolarGrid_12-12_64-64.gif]]

** [2019-06-21 Fri]

*** Cartesian Grid, many large kernels
:LOGBOOK:
CLOCK: [2019-06-21 Fri 09:30]--[2019-06-21 Fri 10:40] =>  1:10
:END:

| Git commit hash | 6389889b78625c940679cf4e066585ded4efb268|
| Grid | Cartesian Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | 5e-1 | 
| Loss | MNIST          | 
| Decoder | 128x16x16 -> 32x8x8 -> 32x4x4

It seems like the digi representations most often occur at the
"center" of the image. I wonder if that's a consequence of the
training data digits being centered by center of mass. Maybe I should
attempt to relocate the MNIST training data (or at least embed on one
side) to compensate for this. 

I'm going to try the cartesian grid, just to see maybe whether this
gives better results (since it covers the whole field rather than just
one hemisphere). 

In terms of separating out the generator - affixing a random seed to
the class labels is possible. 

[2019-06-21 Fri 10:27]
I'm not getting great performance. Maybe large kernels are actually
not a good idea? Perhaps I should go back to smaller (but still
numerous) kernels?

[2019-06-21 Fri 10:35] Out of utter curiosity, I wanted to see what
would happen if I made the learning rate really high (I set it to 1) -
and wow, the loss shot down extremely quickly (about 1e-4 after the
first epoch). The things it produces are...interesting, but not really
recognisable.

Interestingly, I halved it and it shot down even quicker (1e-7 after
the first epoch!). Maybe I can be a bit more aggressive with the
learning rate. 

The digits are still not really recognisable though. They
look...malformed. You can sort of tell that there are features, but
they don't like well put-together. I suppose that's a consequence of
the decoder.

I stopped after 2 epochs because it wasn't really changing after that.

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-21_09-34_loss_CartesianGrid_12-12_64-64.png]]


[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-21_09-34_evolution_CartesianGrid_12-12_64-64.gif]]

*** Cartesian Grid, many smaller kernels
:LOGBOOK:
CLOCK: [2019-06-21 Fri 11:06]--[2019-06-21 Fri 12:04] =>  0:58
:END:

| Git commit hash | 0e3f254e7aa4ff725fdd743cb92505ea7978aa6b|
| Grid | Cartesian Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | Various | 
| Loss | MNIST + Various | 
| Decoder | 64x8x8 -> 64x4x4 -> 64x2x2

From what I can tell, it seems many small kernels works better than
many large kernels. I'll try using smaller kernels this time. 

I'm starting to think I should do a comparison of learning rates. I
feel like I'm going in blind at the moment trying to find what
learning rate is best. 

I might also reduce the batch size so the loss output is a bit more
granular. 

I've tested learning rates of 5e-1, 1e-1 and 5e-2, and the results of
each of these learning rates seem quite similar. And the images don't
really change after the first epoch. For posterity, here's one with a
learning rate of 5e-2: (with a Batch Size of 250 instead of 500). 

[[file:./03-psychophysics/data/archive/2019-06-21-generated-images-epoch-4-cartesian.png]]

I'm going to see what happens if I increaese the Batch Size to 1000,
with a learning rate of 5e-2.

(Results are comparable). 

I'll go back to using 5e-2 with a batch size of 250, and I'm going to
see what happens if I put back in the cosine now (with 10 times
multiplier). 

Here it is after 3 epochs (and it just keeps getting brighter).

[[file:./03-psychophysics/data/archive/2019-06-21-generated-images-epoch-3-cartesian-cosine-10.png]]

I'll try again without the 10 times multiplier. Possibly, the cosine
is dominating the loss and it just ends up trying to optimise for the
cosine function too early.

It produces something pretty similar after 4 epochs, albeit
slower. Maybe I should add another element to the loss function, the
total sum, to steer it away from becoming obscenely bright...

[2019-06-21 Fri 11:24]
They definitelly look a bit cleaner, but it sort of seems they get
trapped in a configuration and don't really change after that. I
wonder how to prevent that from happening. 

[2019-06-21 Fri 11:36]
So, I've now added a small part to the loss function that is simply
the average brightness of each pixel - so it should try and reduce the
brightness of the whole image as part of the loss too now. The output
is interesting, it now feels like it's really trying to limit how many
phosphenes it turns on at once and reduce it to the minimum. Which is
good in some ways since it looks cleaner, but sometimes it does seem
to be remove somethings that seemed initially useful. 

Here's the results after 25 epochs.

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-21_10-42_loss_CartesianGrid_12-12_64-64.png]]


[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-21_10-42_evolution_CartesianGrid_12-12_64-64.gif]]

Probably the best looking digits so far, though this is "easy mode"
considering it's just a 12 x 12 regular cartesian grid (though at
least it does have to cope with the Gaussian blur...and it's producing
a 144 vector only, not the actual image, so there's that too). Loss
decreases below 0 because of course the final output image has pixels
between -1 and 1 and it is not being incorporated into the loss.

I think I'm going to try and repeat this exact configuration, but with
the polar grid now. Though, I have a hunch it will not produce results
looking anywhere near as good as this - partly because I think that
the fact the training digits are centered may be the reason the digits
produced with the polar grid always appear strangely to tend towards
the center too (even though this may not be the best way to represent
it). 

*** Polar Grid, many smaller kernels
:LOGBOOK:
CLOCK: [2019-06-21 Fri 12:59]--[2019-06-21 Fri 14:21] =>  1:22
:END:

| Git commit hash |d0b24265a97e19b4f7f11cd13b1faabb4471ee04 |
| Grid | Polar Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | Various | 
| Loss | Various | 
| Decoder | 64x8x8 -> 64x4x4 -> 64x2x2

So this is the exact same as before, just with a Polar Regular Grid
(and therefore another trained MNIST decoder, but the same
architecture as prior).  - so learning rate of 1e-2.

---

Here's the gif:

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-21_11-47_evolution_PolarGrid_12-12_64-64-with-cosine-and-average-pixel-1e-2.gif]]

It's a shame, it seemed to be doing well, and then I think the average
pixel decided to reduce it too much. I'm going to run it again, but
I'm going to remove the average pixel part of the loss (so otherwise
exact same parameters). 

---

[2019-06-21 Fri 13:36]
This time it sort of went the opposite way - it's too bright now. I'm
going to try just without either of them and see how that goes now too
I think.

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-21_11-47_evolution_PolarGrid_12-12_64-64-with-cosine-only-1e-2.gif]]

The loss curve is pretty similar. It seems like there's a need to
reach a compromise between the two methods. Perhaps there's a better
way than the cosine function and average pixel though...

---

[2019-06-21 Fri 14:02]
I've just tried with /only/ the MNIST decoder i.e. no cosine or
average in sight. There's basically no change after the first few epochs so I'll stop
it after 35. I think it sort of shows though that without /something/, the
contrast is going to be a little bit poor. 

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-21_11-47_evolution_PolarGrid_12-12_64-64-mnist-only-1e-2.gif]]


It strikes me that I'm keeping very poor track of what parameters I'm
testing, I guess because it's so messy and there are simply so many
parameters to tune. At the moment, i've looked at:

1. Number of electrodes in the grid (i.e. vector size)
2. Learning rate
3. Loss functions
4. Decoder architectures
5. Grid types
6. Batch sizes
  
I think there's more to add to that list...but I think there needs to
come a point where I start very deliberately comparing each of
these. I think I started off just exploring because I was still
getting the hang of how to do this at all, but now that I have 'the
hang', I should transition to rigorously testing and comparing these
in more depth. I've recorded all the Git hashes and saved the data,
but I fear this repository is only going to increase in size...

---

As a final check, I'm going to see what happens if I blast the
learning rate up to 1e-1. 

[2019-06-21 Fri 14:10]
Woah, maybe not so final. A loss rate of 1e-1 seems to get much
cleaner digits - you almost can't tell it's a polar grid anymore. 

The loss decreased so quickly after the very first batch...that's interesting.

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-21_11-47_evolution_PolarGrid_12-12_64-64-1e-1.gif]]

The change is so sudden. Unfortunately, I'm not actually recording the
loss before the very first batch - mental note to do this when I start
comparing them rigorously.

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-21_11-47_loss_PolarGrid_12-12_64-64-1e-1.png]]


---

I think I'll try one more time, now with a learning rate of 1. See
what that gets us. 

[2019-06-21 Fri 14:17]
Okay, clearly there's a balance.

I ran it now with a learning rate of 1. 

Here's the output: (no point in a gif, since I stopped it at epoch 2,
I soubted it would change much).

[[file:./03-psychophysics/data/archive/2019-06-21-generated-images-epoch-2-polar-mnist-only-1e-0.png]]

--- 

So I guess the lesson learnt is that I should start keeping track of these variables much more
rigorously. It was fine when I was exploring, but I think I'm ready to
start actually experimentally checking this.

*** Planning 
:LOGBOOK:
CLOCK: [2019-06-21 Fri 14:23]--[2019-06-21 Fri 14:40] =>  0:17
:END:

Just to get things clearer in my head, what I think I can do now is:

1. Tabulate the different parameters for the network, record the loss
   and compare. 

   There are too many dimensions to really compare, so I think I could
   probably just compare:
   1. Learning rate (important hyperparameter that I should
      rationalise)
   2. Loss functions
   3. Decoder architecture
   4. Grid type

   Points 2 and 3 are, what I think, the most important aspects of
   getting the output to look nice. The other parameters (number of
   electrodes, batch size) I think I can keep fairly static. I could
   rationalise using 144 electrodes (12 x 12 grid) by saying that's
   sort of a middle ground between the maximum known before (64) and
   the proposed dimensions of future prosthesis (~400s), on the more
   conservative side. 

   There are 4 dimensions of conditions, all of which are
   important. Hm. 

   If I test 4 grid types, 3 decoder architectures (all
   convolutional), 2 loss functions, and 3 learning rates, that
   gives...72 conditions. I could probably just run them all for 10
   epochs (though I guess learning rate might not be well assessed in
   those situations). So that's about half an hour, times 72, which is
   36 hours. That's doable! I think. 

2. Separate the random generation of digits out from the decoder. 

   I guess the intention with this is that I'm getting digits, but I
   don't know if there are better candidates. This could /actually/
   make the experimental data useful as a refinement technique rather
   than just a testing technique, but I need to think about the
   architecture of how this should be done exactly. In this manner,
   this would be much more like a typical GAN. 

   I think I should decide on this second point before I do the first
   point - because this might be a better way to run it anyway. In
   this sort of paradigm, I guess you could randomise the noise each
   trial, then fix the noise at whatever produces the best
   response. But if the noise is continuous, that's meaningless. Maybe
   I can do this for interest, but it's harder to think exactly about
   how to choose an exactly matching digit. I don't think it's
   meaningful to give people multiple different renders of the same
   thing, which is what the GAN would be able to do.

3. Working with experimental data.

   Considering I've mostly decided on using static digits, maybe this
   isn't such an issue, but I should definitely be recording the
   positions of digits with the experimental data. I guess I am (with
   the mouse position), I should be able to recreate the exact image
   the participant saw at any given point in time. I think I can do
   that already considering the processors are stable and the
   renderers are either stable or loaded from a saved file, and the
   digit positions should be static, but I still think I should
   absolutely triple check this.

   Then I could potentially analyse the differences between conditions
   on a image-feature level by comparing the features and not just the
   results. I don't have a really good idea of what this would look
   like yet, but I think it would be interesting. 

   
I think a combination of 1 and 3 (using the results of 1 to inform 3),
would be the best way to go for now. 2 is better for generating
candidates, but I don't know a good way to select exactly between
candidates when the noise is continuous on many dimensions. 

*** Distorted Polar Grid, same parameters
:LOGBOOK:
CLOCK: [2019-06-21 Fri 14:53]--[2019-06-21 Fri 15:40] =>  0:47
:END:


| Git commit hash |f7c511f72a4e23b133d407f651935349826a92c5 |
| Grid | Distorted Polar Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | Various | 
| Loss | Various | 
| Decoder | 64x8x8 -> 64x4x4 -> 64x2x2

I'm going to just briefly try this again with the distorted polar
grid, just quickly. I don't think I'm going to be able to execute my
plan on a Friday afternoon with my other jobs in mind, so I'll run this
while I multitask. 

Now firstly just running this with only the MNIST decoder, then I'll
have a go at running this with the extra contrast loss measures. 

---

This first run is MNIST-only, 5e-2 learning rate. p

[2019-06-21 Fri 15:13]
It's seems to be getting some...interesting results. Contrast would
really help. 

After 10 epochs, it doesn't really change much.

Here it is:

[[file:./03-psychophysics/data/archive/2019-06-21-generated-images-epoch-10-distorted-polar-mnist-only-5e-2.png]]

Not the  best, but I think there are some visible patterns. The real
issue is with contrast. 

--- 

I'm going to try with a more aggressive learning rate - 1e-1, still
MNIST-only. That seemed to help with contrast even just with the MNIST
decoder on the last one.

[2019-06-21 Fri 15:23]
Loss is about 0.001 after 10 epochs. Here's what it looks like:

[[file:./03-psychophysics/data/archive/2019-06-21-generated-images-epoch-10-distorted-polar-mnist-only-1e-1.png]]

This sort of suggests that I really don't need to be doing more epochs
when the learning rate is this high. 

--- 

I'm now going to try with this same learning rate (1e-1) and with the
cosine added (but not the average pixel). Maybe extra brightness would
be better for this particular grid. Loss got done to about 0.002.

Here it is after 5 epochs:

[[file:./03-psychophysics/data/archive/2019-06-21-generated-images-epoch-10-distorted-polar-mnist-cosine-1e-1.png]]

Interesting, but not much  better.

---

Finally, testing with both the cosine and average pixel loss
additions. 

After the first epoch, loss was already in the negatives. I guess the
MNIST decoder must really just be bale to use these features to get
what it needs. 

Here it is after 3 epochs. Like before, the gray areas just keep
getting dimmer, no drastic changes.

[[file:./03-psychophysics/data/archive/2019-06-21-generated-images-epoch-10-distorted-polar-mnist-cosine-average-1e-1.png]]

Also not terriblly better. I think you can see the features, but
deciphering it without prior knowledge would be hard. Maybe that's
just an intrinsic limitation of the grid. I wonder whether I should be
attempting to train on other datasets other than just MNIST...hmm. 


** [2019-06-24 Mon]

*** Grid and Decoder for Testing Today

| Git commit hash | 5c124635e42766f4fe7e412fe4a8f275f93857f3|
| Grid | Polar Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | 2e-1| 
| Loss | MNIST only | 
| Decoder | 64x8x8 -> 64x4x4 -> 64x2x2

I'm going to rerun the regular polar grid with a learning rate of 1e-1
to test today with the experiment. The digits still are not as
decipherable as I would like still, but I think it can still be used
for testing. 

I chose 2e-1 since it seems that for most of the learning rates above
1e-2, the digits really don't change after the first epoch or so, but
the higher the learning rate, the higher the brightness contrast
(which seems to be beneficial). Here it is with 2e-2 after 3 epochs:

[[file:./03-psychophysics/data/archive/2019-06-24-generated-images-epoch-3-polar-mnist-only-2e-1.png]]

It looked pretty much the same after the first epoch, and didn't
change much after this either.

I just realised though, the grid shouldn't matter for the polar
regular one - they should be exactly the same, so I don't need to keep
saving different ones. That'll save some space, since each one is 10
MB...

For the testing today here are the commands for the regular polar grid:

For the direct processor:

#+BEGIN_SRC 
python digits.py --ncues 25 --ntrials 20 --grid ".\data\training-intermediate-data\training-grids\2019-06-24_09-15_grid_PolarGrid_12-12_64-64.pkl" --processor direct
#+END_SRC

For the net processor:

#+BEGIN_SRC 
python digits.py --ncues 25 --ntrials 20 --grid ".\data\training-intermediate-data\training-grids\2019-06-24_09-15_grid_PolarGrid_12-12_64-64.pkl" --processor net --encoder ".\data\training-intermediate-data\training-encoders\2019-06-24_09-15_encoder_PolarGrid_12-12_64-64.h5"
#+END_SRC

Actually...maybe not. It strikes me that the direct processor is super
easy to decipher.  I guess I can use this if I need to, but I should
really have a go with the distorted grid.

*** Distorted Grid, increasing kernels per layer

| Git commit hash | 1748c847909ce50fda65a8683f0574e6af515f46  |
| Grid | DistortedGrid |
| Vector size | 12 x 12 = 144 | 
| Learning rate |3e-1 | 
| Loss | MNIST only | 
| Decoder |16x8x8 -> 32x4x4 -> 64x2x2 (less dropout) |

I'm going to try increasing the number of kernels with each new layer
for the distorted run. At least I can compare it to the previous
architecure where all layers had the same number of kernels.

It occurred to me that I should try using the digit decoder rather
than the MNIST decoder at some point. Maybe that will be more likely
to get non-handwritten-looking digits...The input shapes are only
different in so far as the digit decoder takes RGB rather than
grayscale values, but it just means there should be an extra
conversion step for the digits. Ah, I guess that also means i need to
train a new decoder with garbage...

I'll leave that for another time I think. 

But anyway, this architecture I've just tried is now a little
different - it has /less/ kernels and smaller dropout. It actually
achieves better accuracy than the previous kernels. 

--

[2019-06-24 Mon 11:24]
It seems to do better. Here are the results after 6 epochs (though it
didn't change much after the first epoch)

[[file:./03-psychophysics/data/archive/2019-06-24-generated-images-epoch-6-distorted-polar-mnist-only.png]]

The loss curve:

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-24_10-25_loss_DistortedPolarGrid_12-12_64-64.png]]

So this afternoon, I'll run for the direct processor:

#+BEGIN_SRC 
python digits.py --ncues 25 --ntrials 10 --grid ".\data\training-intermediate-data\training-grids\2019-06-24_10-25_grid_DistortedPolarGrid_12-12_64-64.pkl" --processor direct
#+END_SRC

And for the net processor:

#+BEGIN_SRC 
python digits.py --ncues 25 --ntrials 10 --grid ".\data\training-intermediate-data\training-grids\2019-06-24_10-25_grid_DistortedPolarGrid_12-12_64-64.pkl" --processor net --encoder ".\data\training-intermediate-data\training-encoders\2019-06-24_10-25_encoder_DistortedPolarGrid_12-12_64-64.h5"
#+END_SRC

( I think I'll just run half the trials). 

Ah...it strikes me this is still much to easy on the regular
grid. Maybe I should try the extra distortions...

*** Reducing the size of experiment window

I think maybe the experiment window being fullscreen is a bit too
much. I think I'll scale it down to one fourth. I guess it's debatable
whether that is representative of what people actually see considering
it's supposed to be similar to the visual fields, but I can change
this later if need be.

*** Rescaling Distorted Grid - 

| Git commit hash ||
| Grid | RescalingDistortedGrid |
| Vector size | 12 x 12 = 144 | 
| Learning rate |2e-2 | 
| Loss | MNIST only | 
| Decoder |32x8x8 -> 48x4x4 -> 64x2x2 (less dropout) |

I've slightly upped the number of kernels. I get the feeling this
particular grid is intrinsically hard, but we'll see. 

I'll save the grid. It's really hard to differentiate between digits..
But interestingly, loss still goes down pretty low.

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-24_11-34_evolution_RescalingDistortedPolarGrid_12-12_64-64.gif]]

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-24_11-34_loss_RescalingDistortedPolarGrid_12-12_64-64.png]]

[2019-06-24 Mon 12:26]
Minor note - it actually does change, very slightly, after the first
few epochs. I guess that's good in a way.

I let it run for 20 epochs.

So here are two new commands for this rescaling distorted grid:

#+BEGIN_SRC 
python digits.py --ncues 25 --ntrials 10 --grid ".\data\training-intermediate-data\training-grids\2019-06-24_11-34_grid_RescalingDistortedPolarGrid_12-12_64-64.pkl" --processor direct
#+END_SRC

And for the net processor:

#+BEGIN_SRC 
python digits.py --ncues 25 --ntrials 10 --grid ".\data\training-intermediate-data\training-grids\2019-06-24_11-34_grid_RescalingDistortedPolarGrid_12-12_64-64.pkl" --processor net --encoder ".\data\training-intermediate-data\training-encoders\2019-06-24_11-34_encoder_RescalingDistortedPolarGrid_12-12_64-64.h5"
#+END_SRC



*** More distortions

I think the DistrotedGrid and RescalingGrid are too similar to the
PolarRegularGrid at the moment.

I've changed the distortion - whereas previously it was a multiplier,
now it's an addition/offset, so phosphenes in the center are also
distorted. I've also more aggreisively changed the rescaling -
phosphene brightness are now to the power of 4, so there should be a
greater difference in values. 


*** Rescaling Distorted Polar Grid with More Distortions

| Git commit hash ||
| Grid | Rescaling Distorted Polar Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | 2e-1 | 
| Loss | MNIST only | 
| Decoder | 32x8x8 -> 48x4x4 -> 64x2x2


We'll see how this goes now with more distortions introduced to the
grid. 

Here are the results:

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-24_12-53_evolution_RescalingDistortedPolarGrid_12-12_64-64.gif]]

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-24_12-53_loss_RescalingDistortedPolarGrid_12-12_64-64.png]]

And to run the grid for the direct processor:

#+BEGIN_SRC 
python digits.py --ncues 25 --ntrials 20 --grid ".\data\training-intermediate-data\training-grids\2019-06-24_12-53_grid_RescalingDistortedPolarGrid_12-12_64-64.pkl" --processor direct
#+END_SRC

And for the net processor:

#+BEGIN_SRC 
python digits.py --ncues 25 --ntrials 10 --grid ".\data\training-intermediate-data\training-grids\2019-06-24_12-53_grid_RescalingDistortedPolarGrid_12-12_64-64.pkl" --processor net --encoder ".\data\training-intermediate-data\training-encoders\2019-06-24_12-53_encoder_RescalingDistortedPolarGrid_12-12_64-64.h5"
#+END_SRC


I tried again after removing the power of the scaler entirely and
making the sizes more random - but I just realised that I need to make
the size a multiple, not an addition, because otherwise the size won't
work properly.

| Git commit hash ||
| Grid | Rescaling Distorted Polar Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | 1e-1 | 
| Loss | MNIST, cosine, average | 
| Decoder | 32x8x8 -> 64x4x4 -> 72x2x2

( Will come back to this, just going to analyse the preliminary data
from Yan )

** [2019-06-25 Tue]

*** Looking at Preliminary Results

Here are some plots of results from the test run yesterday:

#+BEGIN_SRC python :noeval
import glob
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

# DATA READING

data_dir = "./data/psychophysics-sessions/participants/"
data_glob = data_dir + "yw_test*_session.csv"

full_data = [pd.read_csv(f) for f in glob.glob(data_glob)]
session_data = [d[:250] for d in full_data]# Take only the first 250 trials
config_files = [f.replace("_session.csv", "_config.json") for f in glob.glob(data_glob)]

config_data = []

for f in config_files:
    with open(f, 'rb') as infile:
        read = json.load(infile)
        config_data.append(read)

data = list(zip(config_data, session_data))
#+END_SRC

**** Mean Accuracy

#+BEGIN_SRC python :noeval
fig, axes = plt.subplots(1)

colors = ['tomato', 'seagreen']

# d[0] corresponds to config, d[1] corresponds to session data
accuracies = [sum(d[1]['digit'] == d[1]['keypress'])/len(d[1]) * 100 for d in data]
labels = [f"{d[0]['PROCESSOR_TYPE']}" for d in data]
          
plt.bar(range(len(data)), accuracies, align='center', color=colors, tick_label=labels)
plt.ylim((0,100))
plt.title("Mean Digit Classification Accuracy by Processing Method\n{}".format("yw_test"))
plt.ylabel("Percentage Accuracy")
plt.axhline(10, color='r', linestyle=':')
#+END_SRC

[[file:./03-psychophysics/data/archive/2019-06-25-mean-accuracy-yw-test.png]]

**** Mean Response Time

#+BEGIN_SRC python :noeval
fig, axes = plt.subplots(1)

# d[0] corresponds to config, d[1] corresponds to session data
times = [d[1]['cuetime'].mean() for d in data]
stds = [d[1]['cuetime'].std() for d in data]
          
plt.bar(range(len(data)), times, align='center', color=colors, tick_label=labels, yerr=stds)
plt.title("Mean Time to Classification per Cue by Processing Method\n{}".format("yw_test"))

plt.ylabel("Time to Classification (seconds)")
#+END_SRC

[[file:./03-psychophysics/data/archive/2019-06-25-mean-response-time-yw-test.png]]

**** Confusion Matrix

#+BEGIN_SRC python :noeval
from sklearn.metrics import confusion_matrix
import seaborn as sn

fig, axes = plt.subplots(1, len(data), figsize=(12, 4))

confusion_matrices = [confusion_matrix(d[1]['digit'],
                                       d[1]['keypress'],
                                       labels=range(10))
                      for d in data]

global_max = np.max([np.max(cm) for cm in confusion_matrices])

for i in range(len(data)):
    sn.heatmap(confusion_matrices[i],
               annot=True, 
               ax=axes[i],
               vmin=0,
               vmax=global_max,
               cbar=False)
    axes[i].set(xlabel='Predicted digit', ylabel='True digit', title="Confusion matrix, {}".format(labels[i]))

fig.suptitle("yw_test")
#
#+END_SRC

[[file:./03-psychophysics/data/archive/2019-06-25-confusion-matrix-yw-test.png]]


**** Rolling Average Accuracy

#+BEGIN_SRC python :noeval
correct = [full_data[i]["digit"] == full_data[i]["keypress"] for i in range(len(data))]

fig, ax = plt.subplots(1)

for i in range(len(correct)):
    plot_data = correct[i].rolling(window=25, center=True).mean()
    plt.plot(plot_data, c=colors[i], label=data[i][0]["PROCESSOR_TYPE"])

plt.legend()
plt.axhline(0.1, c='red', linestyle=':')

    
fig.suptitle("Rolling Mean (window = 25), yw_test")
#+END_SRC


[[file:./03-psychophysics/data/archive/2019-06-25-rolling-average-yw-test.png]]


**** Summary

I'm not really surprised by the results - the way I rendered the grid
changed between when I did the experiment and now, and the distortion
now is comparatively minimal. When there is such minimal distortion,
a masking approach is definitely easier. I think experimentally, it
will be important to show what distortions a masking approach can and
cannot cope with.

*** Adding further distortions

I have made the size multiplier larger (random float between 0 and 3)
and clipped the rescaling grid below 0.5, so now the conditional-ism
of the grid is greater. 

I have also changed the =digits.py= script to ensure two of the same
digit do not occur in a row. 

*** Using MNIST digits instead of Font Digits for the Experiment?

I wonder if perhaps it would be fairer to compare using the MNIST
digits as the "underlying base" of the digit experiment, rather than
using the font digits directly. It feels more like you are directly
comparing processors in this case, because it's possible the MNIST
digits are less recognisable than the computer font digits. The only
issue is, I feel that the digits have to be equivalently learnable in
both conditions - i.e. digits must be consistent, which clearly cannot
be the case when using MNIST digits as the digits all vary.

*** Adding Computer Font Digits to Training

I might try adding the computer font digits to the training and see
whether that might produce more recognisable patterns. 

[2019-06-25 Tue 10:46]
I've added the font digit images to the training data so we'll see how
it performs.

I think the other issue with what's produced at the moment is...it's
just not clean. There are a lot of extraneous phosphenes. Lines are
not well-joined. I'm not entirely sure how I can combat that -
previously I've been trying to do so in the loss function, but perhaps
this is something that would be better incorporated into the decoder. 

Maybe there needs to be more of an adversarial component - like the
decoder should learn more garbage as it's being produced. 

*** Cartesian and Font Digits with Training


| Git commit hash | 27c77120691a759cc1af2d550db32608ea3bc159|
| Grid | Cartesian Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | 5e-1 | 
| Loss | MNIST  with Font| 
| Decoder | 32x8x8 -> 64x4x4 -> 72x2x2|

The first thing  I'm going to try is with the cartesian grid again.

...the computer froze. Unfortunately, I have to install everything
again which is a pain. And tensorflow is a little fickle with
installation...Had a problem with DLL load failing with tensorflow, so
I reinstalled using conda using =conda install tensorflow-gpu= and
then installit ng tensorflow-2.0.0 with =pip install
tensorflow==2.0.0-beta1=, and that seems to have worked. 

[2019-06-25 Tue 12:46] The results are not particularly good. Here it
is after 3 epochs (loss goes down to 4e-05!)

[[file:./03-psychophysics/data/archive/2019-06-25-generated-images-epoch-3-mnist-combined-cartesian.png]]

I'm just going to try with only the font images and garbage now..then
I think I need to make a bit of a stronger decision about what to do.

*** Cartesian and Only Font Training Digits

| Git commit hash | |
| Grid | Cartesian Grid |
| Vector size | 12 x 12 = 144 | 
| Learning rate | 5e-2 | 
| Loss | Font (no MNIST)  | 
| Decoder | 32x8x8 -> 64x4x4 -> 72x2x2|

I'm not expecting this run to actually perform that well, considering
I think the whole point was that the MNIST dataset is supposed to be
fairly generic and extract features rather than digits themselves
(which I would guess is going to happen for this font decoder). Still,
I think it would be interesting to test. 

...THe computer almost froze again when I was trying to run this. I
have a feeling it's the "make garbage" - probably too much is being
made and it has to hold all of it in memory. I'll reduce it down to a tenth. 

[2019-06-25 Tue 13:15]
I'm not sure whether this is a consequence of having trained on less
garbage samples, but the output is definitely not very good. 

Here it is:

[[file:./03-psychophysics/data/archive/2019-06-25-generated-images-epoch-5-font-only.png]]

I'll probably just stick to MNIST...

*** Attempt to Train Decoder Garbage

| Git commit hash | 6316d106c3ff66f3de1317b5f8bec04d1a9f9313  | 
| Grid | Cartesian Grid | 
| Grid Size | 12 x 12 = 144 | 
| Testing | Adversarial network with decoder in training (pretrained) |    

So I'm going to try to train the decoder to better recognise garbage
/during/ the encoder training. I think it's going to be very hard to
get a good balance though, considering the decoder is
"pretrained"...but actually, maybe I shouldn't pretrain the decoder on
garbage if I'm going to train it later?

On my first try, I set:

| Encoder learning rate | 5e-2 | 
| Decoder learning rate | 1e-2 | 

I've finally managed to hook things up with the adversarial
network. The first run is..interesting. The digits actually train more
than just after the first epoch now, which I think is good - so I
probably shouldn't stop after one epoch anymore. 

But it's very interesting - it gets better, and then it gets
worse. I'm not sure why - I mean I guess it makes sense that it starts
seeing the "good" encoded digits as garbage, but if it gets too far
away from MNIST, I would assume that it would have to pull back again
because then it would have an easy way to distinguish between garbage
and MNIST. 

I guess it's worth noting as well that this decoder was
pretrained. Perhaps I should try with a non-trained network..and
possibly a slower learning rate for both.

Here's the gif:

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-25_13-18_evolution_CartesianGrid_12-12_64-64.gif]]


And the loss curves, now with both the training losses plotted:

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-25_13-18_loss_CartesianGrid_12-12_64-64.png]]
 
*** Adversarial - no pretrained network, cartesian

| Git commit hash |3654bfc650b4f5189177bbb4334ac2f2b185774e  | 
| Grid | Cartesian Grid | 
| Grid Size | 12 x 12 = 144 | 
| Testing | Adversarial network with no pretraining of decoder  |

I'm going to try the same thing - except the decoder will be trained
from scratch during the encoder training. We'll see how it goes. 

| Encoder learning rate | 1e-2  |
| Decoder learning rate | 1e-2 | 

Because the decoder is now naive, I'm setting the decoder learning
rate to the same as the encoder learning rate. 

[2019-06-25 Tue 15:39]
I've stopped it after Epoch 26. It takes a while, but it gets (what
seems to be, to me at least) fairly acceptable results even without
pretraining. It might get better with time if I went for more epochs.

Evolution:

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-25_14-47_evolution_CartesianGrid_12-12_64-64.gif]]

At epoch 26:

[[file:/03-psychophysics/data/archive/2019-06-25-generated-images-epoch-26-adversarial-cartesian.png]]

And loss curves:

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-25_14-47_loss_CartesianGrid_12-12_64-64.png]]

It seems like the decoder is still training a bit fast. Maybe it could
be a little slower than the encoder? 

But I think I'd like to try this for the polar grid now.

*** Adversarial - no pretrained network, polar
    
| Git commit hash | a2e54559ae62c0b642a5f563c906ee2a87f525d5 | 
| Grid | Polar Grid | 
| Grid Size | 12 x 12 = 144 | 
| Testing | Adversarial network with no pretraining of decoder for a polar grid |

Exactly the same as before, but just a different grid now.

| Encoder learning rate | 5e-2  |
| Decoder learning rate | 1e-2 | 

First attempt doesn't look particularly nice at all. I think the fact
that digits are centered in MNIST might have a role. I'll see if I can
shift the images a little bit to the right.

[2019-06-25 Tue 16:11]
I've made the MNIST sizes the same as the original and I've rolled
them to the right by a quarter of the image dimension. Going to
rerun - let's see how it goes.

I'll also try increasing the learning rate of the encoder to 5e-2.

[2019-06-25 Tue 22:24]
I can barely see the digits - I'll try upscaling the MNIST digits that
it's trained on so they're not too thin.

(There were a few specks of light in the middle, but definitely not
full digits or anything like it). 

[2019-06-26 Wed 09:36]
It's interesting - it seems to look most "digit-like" at Epoch 10,
then sort of degrades afterwards. 

Here it is at Epoch 10:

[[file:./03-psychophysics/data/archive/2019-06-26-generated-images-epoch-10-polar-adversarial.png]]

And at epoch 50:

[[file:./03-psychophysics/data/archive/2019-06-26-generated-images-epoch-50-polar-adversarial.png]]

The full training gif:

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-25_15-42_evolution_PolarGrid_12-12_64-64.gif]]

And the loss curve, though it's sort of hard to see:

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-25_15-42_loss_PolarGrid_12-12_64-64.png]]

I would like to see how this fares for the distorted grid.

*** Adversarial - no pretrained network, distorted polar
  
| Git commit hash | | 
| Grid | Distorted Polar Grid | 
| Grid Size | 12 x 12 = 144 | 
| Testing | Adversarial network with no pretraining of decoder for a distorted polar grid |

[2019-06-26 Wed 10:34]
It's very interesting what happens during training - it seems like it
was not going to change very much after the first few epochs...then
all of a sudden, it changes drastically and both encoder and decoder
loss skyrocket. 

[2019-06-26 Wed 11:45]
I'm going to increase the buffer size to 60,000 in line with the MNIST
grid, so each EPOCH should go through the whole MNIST data. I'm also
going to change the decoder to network to use smaller kernels.

It takes so long to generate the digit images for input, I think I'm
just going to have to repeat the digit images to fill the MNIST
space of 60000 if I want to use the full MNIST dataset.

The greater number of image will definitely increase the training time
to about 600 seconds or ten minutes an epoch...we'll see how it goes. 

[2019-06-26 Wed 12:04]
I got a message bout exceeding 10% of system memory with
tensorflow. Well...I guess I think I'll just try half the dataset for
now and see how it goes.

[2019-06-26 Wed 13:13]
Smaller kernels do not work as well for this particular grid. I'll go
back to a 64x8x8 -> 128x4x4 model.

[2019-06-26 Wed 13:37]
This didn't work particularly well. Here it is at epoch 11:

[[./03-psychophysics/data/archive/2019-06-26-generated-images-epoch-11-distorted-polar-adversarial.png]]

(Learning rate was 5e-2 for both encoder and decoder). 

*** Adversarial - no pretrained network, polar, slower learning rate

Out of interest, I want to see what happens if the learning rate is
much smaller for the polar grid - say, 5e-3 for both the encoder and decoder.

| Git commit hash |f59f6cb27c8037be461687314a465313b1989323 | 
| Grid | Polar Grid | 
| Grid Size | 12 x 12 = 144 | 
| Testing | Slower adversarial learning rate |

Learning rate of both encoder and decoder are 1e-3.

[2019-06-27 Thu 09:53]
This training run went well I think. I've saved the encoder - I think
I would need to demonstrate parity with the direct processor for this
particular regular grid.

I tested whether it would run with the digit experiment, and it
does. I don't need to save the grid as it's the same polar grid as the
others.

Here's a command to run it:

#+BEGIN_SRC 
python digits.py --grid .\data\training-intermediate-data\training-grids\2019-06-24_09-15_grid_PolarGrid_12-12_64-64.pkl --testing --encoder .\data\training-intermediate-data\training-encoders\2019-06-26_13-39_encoder_PolarGrid_12-12_64-64.h5 --processor net
#+END_SRC

Here's the training gif:

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-26_13-39_evolution_PolarGrid_12-12_64-64.gif]]

And the loss curves:

[[file:./03-psychophysics/data/training-intermediate-data/training-graphs/2019-06-26_13-39_loss_PolarGrid_12-12_64-64.png]]

The comparison with direct brightness (clearly very good for this
particular grid since it is regular)

[[file:./03-psychophysics/data/archive/2019-06-26-comparison-direct-digits-polar.png]]

For this particular grid, I don't think I should be aiming to
demonstrate that the learned version is /better/ (because it won't
be) - I think I should just aim to demonstrate parity. The other grids
will be more interesting to compare for an actual improvement. 


**  [2019-06-27 Thu]

*** Rescaling Distorted Grid - Same Parameters

I'm going to test now on the rescaling distorted polar grid, which is
the most distorted grid and the one whose distortions would be most
interesting to overcome. 

I'm using the same parameters as before for the polar grid. 

[2019-06-27 Thu 10:31]
Performance is not particularly good with the clipping. I think I need
to have some way to guarantee that at least /something/ should be
possible to make - maybe do a conditional version of the regular polar
grid and distorted grid separately. 

[2019-06-27 Thu 10:39]
After 10 epochs, I don't see much improvement. I think I'll try the
conditional on the regular polar grid first actually.

Argh. I just realised for the rescaling paradigm, that I was
normalising the renders anyway for the grid. I've removed that now.

*** Rescaling Polar Grid - Same Parameters

| Git commit hash | 9a497bd7d7c3a9acc4f4efba16d682c5f75aa737 |
| Learning rate | Various | 
| Grid | Rescaling Polar Grid | 
| Grid Size | 12 x 12 = 144 | 
    
This time, purely testing whether the network can withstand the
conditional distortion. This isn't the most "interesting" part -
really, it's just a problem of reconfiguring weights - but we'll see
anyway. 

[2019-06-27 Thu 11:12]
At the learning rate of 1e-3 for both the encoder and decoder, the
encoder was getting sort of stuck around a loss of 10 for a while. I
decided to up the learning rate of the encoder to 1e-2 and keep the
decoder at 1e-3. 

[2019-06-27 Thu 13:06]
This appeared to do much better - but it seemed best after the second
epoch, and actually seems to do worse with more epochs. 

Here it is at epoch 11:

[[file:./03-psychophysics/data/archive/2019-06-27-generated-images-epoch-11-rescaling-polar.png]]

And at epoch 38:

[[file:./03-psychophysics/data/archive/2019-06-27-generated-images-epoch-38-rescaling-polar.png]]

For comparison, here are the digits run through a direct
brightness-based decoder: 

[[file:./03-psychophysics/data/archive/2019-06-26-comparison-direct-digits-rescaling-polar.png]]

I'm going to rerun this now again with slower learning rates: 5e-3 for
the encoder and 5e-4 for the decoder.

[2019-06-27 Thu 14:29]
Same sort of scenario - it seems to degrade with more epochs after a
certain point. I wonder whether it's because the decoder surpasses the
training of the encoder. 

Here's the training gif for this particular run with the slower
learning rates:

[[file:./03-psychophysics/data/training-intermediate-data/training-gifs/2019-06-27_10-53_evolution_RescalingPolarGrid_12-12_64-64.gif]]

*** Distorted Polar Grid - Same Parameters

| Git commit hash |     |
| Grid | Distorted Polar | 
| Grid Size | 8 x 8 = 64 | 
| Learning rate encoder | 5e-3 | 
| Learning rate decoder | 5e-4 | 

Now attempting to run this on the distorted grid. I think spatial
distortions, which are the most interesting to overcome, are also
going to be the hardest. Maybe I should remove the brightness
distortion and have it purely spatial distortion - that might remove
some of the difficulty for the encoder, which is going against a
decoder that has MNIST values that are starkly white. 

[2019-06-27 Thu 15:04]
I think I might just run this with ONLY location distortion
actually. Size and strength, I'll test separately.

I'm also going to try this out on the 8x8 grid - I think having less
phosphenes probably a more likely case in which a generated encoding
would be better than a direct encoding, since there might be some
extra changes that are needed to skirt around the low resolution.

[2019-06-28 Fri 09:42]
Didn't work particularly well. Encoder loss was around 20 at the
end. Some of the features are present, but they don't really form
holistic digits. I wonder if the learning rate needs to be faster. 

Here it is at epoch 50:

[[file:./03-psychophysics/data/archive/2019-06-28-generated-images-epoch-50-distorted.png]]


*** Aside : Seeded Generative Network

I wonder if perhaps a generative network with a random seed could be
useful to produce many candidates...

Which could then be put into the digit experiment to gather *vector*
data and their *classifications* as digits. So this would be a /more/
phosphene agnostic method (though it still relies on being able to
produce somewhat meaningful candidates). 

Then, the vector data and their classifications could be used for
/feature extraction/ - i.e. extracting features from the (arbitrary!)
vectors that best correlate with digits. 

I suppose I could just make this an add-on to the experiment plan so
far - but it makes the amount of analysis afterwards quite
significant. It would be interesting to see the results though (though
I don't imagine they would look particularly good). 

** [2019-06-28 Fri]

*** Seeded Generative Network

I might just try this - I think it would be an interesting angle to
approach it from. 

I'm also going to try reducing the number of phosphenes down to 64,
which I think is the more interesting case for a limitation. I've
changed the value of a to double its prior value to compensate for the
decrease in phosphene density, so there will be more likelihood of
connection between phosphenes.  

Essentially, this will be an architecture like:

#+BEGIN_SRC 
32 vector seed + 10 one-hot category 
-> [ENCODER] 
-> 64 vector. 
-> [RENDERER]
-> 64 x 64 image
-> [DECODER]
-> 11 category softmax
#+END_SRC

To train the encoder, the classification output of the decoder will be
compared to a one-hot of the original input. The train the decoder,
the classification output of the decoder will be compared to a one-hot
of the garbage element. 

[2019-06-28 Fri 10:21]
Okay, I've set it up using a 32 vector seed with a single hidden layer
of half the final output layer. Not sure how this'll go, but we'll
see. I've deliberately kept the learning rate very low, 5e-3 for the
encoder and 5e-4 for the decoder. I've also upped the buffer size to
30000 MNIST images to make use of more of the dataset.

| Grid | Polar Regular Grid | 
| Vector length | 8 x 8 = 64 |
| Encoder learning rate | Various |
| Decoder learning rate | Various | 

Training is going slowly, but I do think it is gradually getting
better - though, granted, I can only see one particular set of digits,
rather than the whole set. 

[2019-06-28 Fri 11:59]
It generally seems like it is unstable - probably because there is no
configuration that will completely minimise the loss for encoder
training, simply because the map is imperfect (i.e. there are some
places where you simply cannot join lines, resulting in a disjointed
figure). I wonder if perhaps the style transfer idea on the MNIST
digits isn't such a bad idea after all. There needs to be something
that will account for the difference between the intrinsic ability of
the map to represent digit forms, and the MNIST digits themselves. 

But then again, if the role of this phase is only to produce
candidates for the next phase of actual presentation to humans, then
maybe it isn't such an issue. 

I notice the encoder loss is bouncing up and down between 10 and 14. 

I've just taken a quick look at the generated digits for different
random noise and...there's no variation for a single
digit. Huh. Perhaps the conditional nature needs to be input in a
different way. I supppose this is mode collapse...

Perhaps I should increase the encoder training further relative to the
decoder training. I'll have a go at using 5e-2 and 5e-4 - so a factor
of 2 of difference. 


[2019-06-28 Fri 13:03]
No, that was equally unsuccessful, if not worse. It just stopped
changing completely after the 10th epoch and encoder loss again just
hovered around the 10-15 mark. There was also no variation in
digits...

Ah, maybe I should keep the learning rates as similar as possible I'll
try a learning rate of 1e-3 for both the encoder and decoder this
time. 

[2019-06-28 Fri 13:40]
With a learning rate of 1e-3, it seems equally to end up reaching a
point where the encoder loss hovers around 10ish and doesn't seem to
go down significantly from there. It seems I will definitely need some
strategy to try and minimise that. 

There is also no variation with this configuration either...

I might just try with a cartesian grid with this random seed
approach. 

[2019-06-28 Fri 13:51]

| Grid                  | Cartesian Grid |
| Vector length         | Cartesian Grid |
| Encoder learning rate | Various        |
| Decoder learning rate | Various        |

I'll start with a learning rate of 1e-3 for both the encoder and
decoder. 

[2019-06-28 Fri 14:06]
After 3 epochs, there is still very noticeable mode collapse going
on. 

[2019-06-28 Fri 14:45]
Attempting to increase the decoder learning rate to 1e-2 and keeping
the encoder learning rate at 1e-3 produced poor results - the
encodings almost disappeared by epoch 8. 

*** Directions

Here's a few things that might help:

1. Size - I'm not sure whether there's a good way to do this, but
   potentially training on digits of many different sizes would be
   more successful so the produced images weren't limited to a single
   size.
2. Style transfer of phosphenes -> at least having a comparison that
   isn't completely as perfect as the MNIST dataset. 

I don't know if mode collapse is really a problem for the eventual
goal (namely to simpy choose one /best/ representation), but it
severely limits my ability to use the experimental phase as anything
but evaluation. I would really like to be able to have candidates that
can be tested for the experimental phase, but clearly I need to some
way of reducing the mode collapse...


** [2019-07-01 Mon]

*** Polar Seeded Run
    :LOGBOOK:
    CLOCK: [2019-07-01 Mon 09:20]--[2019-07-01 Mon 10:14] =>  0:54
    :END:


I'm going to try a polar seeded run, for interest's sake, with 12 x 12
phosphenes (so it should be fairly high resolution...).

This paper:

https://openreview.net/pdf?id=rJliMh09F7

Suggests that approaching mode collapse by incorporating it into the
loss function may be viable. 

[2019-07-01 Mon 09:39]
Still getting noticeable mode collapse for this run. 

[2019-07-01 Mon 09:59]
I've implemented a very basic attempt at trying to increase the
difference between encodings - essentially, by doing a matrix
multiplication between the encoded vectors and itself, then comparing
to elementwise square of each row, broadcasted. I just realised though
that this may not actually work since each batch contains all 10
digits (so the difference is going to be large anyway)...

[2019-07-01 Mon 10:13]
Well, there's still mode collapse - the noise still produce the same
digits. That's a real pity. I might leave this for now then and go
back to the decoder-attached model training.

*** 

* July 2019
 
* August 2019
* September 2019

   #+begin_quote
   [2019-09-25 Wed] - Department Oral No. 2
   #+end_quote

* October 2019

   #+begin_quote
   [2019-10-29 Tue] - Thesis Due

   [2019-10-29 Tue] - Faculty Oral and Poster upload to Moodle Due

   [2019-10-30 Wed] and [2019-10-31 Thu] - Faculty Oral and Poster Days
   #+end_quote


